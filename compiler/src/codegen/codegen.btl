module codegen;

import "../hir/hir";
import "../hir/hir_lower";
import "../semantics/core";
import "../semantics/type";
import "../semantics/sym";
import "../support/libc";
import "../support/utils";
import "../syntax/ast";
import "asm";
import "call_conv";

/// Note: [Frame layout]
/// ~~~~~~~~~~~~~~~~~~~~
/// The code generator organizes the stack frame as follows:
/// ```
/// +-------------------+
/// |                   |
/// | spills            |
/// |                   |
/// +-------------------+ <- FP
/// | saved x8          |
/// +-------------------+
/// |                   |
/// | saved varargs     |
/// |                   |
/// +-------------------+
/// |                   |
/// | slots             |
/// |                   |
/// +-------------------+
/// |                   |
/// | scratch           |
/// |                   |
/// +-------------------+
/// |                   |
/// | outgoing args     |
/// |                   |
/// +-------------------+ <- SP
/// ```

//==============================================================================
//== Helper Functions

func reg_is_callee_saved(reg: Reg): Bool {
    return Reg_X19 <= reg && reg <= Reg_X28;
}

func is_int_or_enum(t: *Type, max_size: Int): Bool {
    match (t.kind) {
        case Type_Int:
            return (t as *IntType).size <= max_size;
        case Type_Enum:
            return (t as *EnumType).sym.size <= max_size;
        case _:
            return false;
    }
}

//==============================================================================
//== Codegen Context

struct RegReservation {
    did_spill: Bool,
}

struct RegState {
    reservations: *mut List, // List<RegReservation>
    is_locked: Bool,
    n_writes: Int,
}

struct LoopCtx {
    step_label: Int,
    done_label: Int,
}

struct Slot {
    fp_offset: Int,
}

struct CodegenCtx {
    out: *mut File,

    // execution context
    current_func: *FuncSym,
    current_loop: LoopCtx,
    current_call_layout: CallLayout,

    // varargs area
    saved_varargs_gr_top_fp_offset: Int,
    saved_varargs_gr_size: Int,

    // slot area
    n_slots: Int,
    slots: *Slot,

    // scratch area
    scratch_fp_offset: Int,
    current_scratch_size: Int,
    max_scratch_size: Int,

    // argument passing area
    max_arg_pass_size: Int,

    // labels
    n_labels: Int,
    ret_label: Int,

    // strings
    strings: *mut List, // List<*StringBuffer>

    // asm builder
    builder: *mut AsmInstrBuilder,

    // register allocation
    regs: [RegState; N_REGS],
}

func next_label(ctx: *mut CodegenCtx): Int {
    var label = ctx.n_labels;
    ctx.n_labels += 1;
    return label;
}

func define_global_string(ctx: *mut CodegenCtx, value: *StringBuffer, xd: Reg): Int {
    list_push(ctx.strings, value);
    return list_len(ctx.strings) - 1;
}

//==============================================================================
//== Codegen

func write_push(ctx: *mut CodegenCtx, reg: Reg) {
    ctx.current_scratch_size += 8;
    ctx.max_scratch_size = int_max(ctx.max_scratch_size, ctx.current_scratch_size);

    // str {reg}, [sp, #{stack_size + fp_offset}]
    var fp_offset = ctx.scratch_fp_offset - ctx.current_scratch_size;
    asm_write_str_r_sp_from_fp(ctx.builder, 8, xs: reg, fp_offset: fp_offset);
}

func write_pop(ctx: *mut CodegenCtx, reg: Reg) {
    // ldr {reg}, [sp, #{stack_size + fp_offset}]
    var fp_offset = ctx.scratch_fp_offset - ctx.current_scratch_size;
    asm_write_ldrs_r_sp_from_fp(ctx.builder, 8, reg, fp_offset);

    ctx.current_scratch_size -= 8;
}

func lock_reg(ctx: *mut CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    state.is_locked = true;
    state.n_writes += 1;
}

func unlock_reg(ctx: *mut CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    assert(state.is_locked, "unlock_reg: register should be locked.");
    state.is_locked = false;
}

func spill_reg(ctx: *mut CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    assert(state.is_locked, "spill_reg: register should be locked.");
    write_push(ctx, reg);
    unlock_reg(ctx, reg);

    asm_add_comment_to_last(ctx.builder, "8 byte spill");
}

func unspill_reg(ctx: *mut CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    assert(!state.is_locked, "unspill_reg: register should not be locked.");
    write_pop(ctx, reg);
    lock_reg(ctx, reg);

    asm_add_comment_to_last(ctx.builder, "8 byte reload");
}

func reserve_reg(ctx: *mut CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    var spill_needed = state.is_locked;
    if (spill_needed) {
        spill_reg(ctx, reg);
    }

    var reservation = box(sizeof(RegReservation), &RegReservation { did_spill: spill_needed }) as *RegReservation;
    list_push(state.reservations, reservation);
}

func free_reg(ctx: *mut CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    var reservation = list_pop(state.reservations) as *mut RegReservation;
    var did_spill = reservation.did_spill;
    free(reservation);

    if (state.is_locked) {
        unlock_reg(ctx, reg);
    }
    if (did_spill) {
        unspill_reg(ctx, reg);
    }
}

func next_reg_except_2(ctx: *mut CodegenCtx, x1: Reg, x2: Reg): Reg {
    /*
        Register usage:
        - SP The Stack Pointer.
        - r30 / LR - The Link Register.
        - r29 / FP - The Frame Pointer
        - r19...r28 Callee-saved registers
        - r18 The Platform Register.
        - r17 IP1 The second intra-procedure-call temporary register.
        - r16 IP0 The first intra-procedure-call scratch register.
        - r9...r15 Temporary registers
        - r8 Indirect result location register
        - r0...r7 Parameter/result registers
    */

    var priority: [Reg; 30] = [
        Reg_X0, Reg_X1, Reg_X2, Reg_X3, Reg_X4, Reg_X5, Reg_X6, Reg_X7,
        Reg_X8,
        Reg_X9, Reg_X10, Reg_X11, Reg_X12, Reg_X13, Reg_X14, Reg_X15,
        Reg_X19, Reg_X20, Reg_X21, Reg_X22, Reg_X23, Reg_X24, Reg_X25, Reg_X26, Reg_X27, Reg_X28,
        Reg_X16, Reg_X17, Reg_X18,
        Reg_X30,
    ];

    for (var i = 0; i < 30; i += 1) {
        var reg = priority[i];
        if (reg != x1 && reg != x2) {
            var state = &ctx.regs[reg as Int];
            if (!state.is_locked) {
                reserve_reg(ctx, reg);
                return reg;
            }
        }
    }

    unreachable("next_reg_except_2: no free registers available.");
}

func next_reg_except_1(ctx: *mut CodegenCtx, x1: Reg): Reg {
    return next_reg_except_2(ctx, x1, x2: x1);
}

func next_reg(ctx: *mut CodegenCtx): Reg {
    return next_reg_except_2(ctx, x1: SP, x2: SP);
}

func spill_before_call(ctx: *mut CodegenCtx) {
    for (var i = 0; i < N_REGS; i += 1) {
        if (!reg_is_callee_saved(i as Reg)) {
            reserve_reg(ctx, i as Reg);
        }
    }
}

func unspill_after_call(ctx: *mut CodegenCtx) {
    for (var i = N_REGS - 1; i >= 0; i -= 1) {
        if (!reg_is_callee_saved(i as Reg)) {
            free_reg(ctx, i as Reg);
        }
    }
}

func get_fp_offset_for_slot(ctx: *mut CodegenCtx, type: *Type, slot_id: Int): Int {
    return ctx.slots[slot_id].fp_offset;
}

// str {reg}, [fp, #{fp_offset + offset}]
func write_slot_store_partial(ctx: *mut CodegenCtx, type: *Type, slot_id: Int, offset: Int, reg: Reg) {
    var width = type_size(type);
    var fp_offset = get_fp_offset_for_slot(ctx, type, slot_id);
    asm_write_str_ri(ctx.builder, width, reg, FP, fp_offset + offset);
}

// str {reg}, [fp, #{fp_offset}]
func write_slot_store(ctx: *mut CodegenCtx, type: *Type, slot_id: Int, reg: Reg) {
    write_slot_store_partial(ctx, type, slot_id, 0, reg);
}

// add {reg} fp, #{fp_offset}
func write_slot_addr(ctx: *mut CodegenCtx, type: *Type, slot_id: Int, reg: Reg) {
    var width = type_size(type);
    var fp_offset = get_fp_offset_for_slot(ctx, type, slot_id);
    asm_write_add_ri(ctx.builder, width, reg, FP, fp_offset);
}

// ldr {reg}, [sp, #{stack_size + fp_offset}]
func write_slot_read(ctx: *mut CodegenCtx, type: *Type, slot_id: Int, xd: Reg) {
    var width = type_size(type);
    var fp_offset = get_fp_offset_for_slot(ctx, type, slot_id);
    asm_write_ldrs_r_sp_from_fp(ctx.builder, width, xd, fp_offset);
}

func write_sign_extend(ctx: *mut CodegenCtx, source: *Type, xd: Reg, x1: Reg) {
    assert(is_scalar(source), "write_sign_extend: source should be a scalar.");
    if (is_int_or_enum(source, max_size: 4)) {
        var target_size = 8;
        var source_size = type_size(source);
        // {sxt} {xd}, {x1}
        asm_write_sxt(ctx.builder, target_size, source_size, xd, x1);
    } else {
        if (xd != x1) {
            // mov {xd}, {x1}
            asm_write_mov_r(ctx.builder, 8, xd, x1);
        }
    }
}

func asm_lower_expr_binary(ctx: *mut CodegenCtx, op: *Char, e1: *HirExpr, e2: *HirExpr, xd: Reg) {
    var x1 = next_reg_except_1(ctx, xd);

    // xd <- ...
    asm_lower_expr(ctx, e1, xd);
    // x1 <- ...
    asm_lower_expr(ctx, e2, x1);
    // {op} xd, xd, x1
    asm_write_binary_op_rr(ctx.builder, op, xd, xd, x1);

    free_reg(ctx, x1);
}

func asm_lower_expr_cmp(ctx: *mut CodegenCtx, op: *Char, e1: *HirExpr, e2: *HirExpr, xd: Reg) {
    var x1 = next_reg_except_1(ctx, xd);

    // xd <- ...
    asm_lower_expr(ctx, e1, xd);
    // x1 <- ...
    asm_lower_expr(ctx, e2, x1);
    // cmp xd, x1
    asm_write_cmp_rr(ctx.builder, 8, xd, x1);
    // cset xd, op
    asm_write_cset(ctx.builder, 8, xd, op);

    free_reg(ctx, x1);
}

func asm_lower_expr_addr(ctx: *mut CodegenCtx, e: *HirExpr, xd: Reg) {
    match (e.kind) {
        case HirExpr_Var if (e as *HirVarExpr).sym.kind == Sym_Local: {
            var e = e as *HirVarExpr;
            var sym = e.sym as *LocalSym;

            if (sym.is_indirect) {
                write_slot_read(ctx, sym.type, sym.slot_id, xd);
                asm_add_comment_to_last(ctx.builder, "local '%s' (indirect) @ %d", sym.name, sym.slot_id);
            } else {
                write_slot_addr(ctx, sym.type, sym.slot_id, xd);
                asm_add_comment_to_last(ctx.builder, "local '%s' @ %d", sym.name, sym.slot_id);
            }
        }
        case HirExpr_Var if (e as *HirVarExpr).sym.kind == Sym_Global: {
            var e = e as *HirVarExpr;
            var sym = e.sym as *GlobalSym;
            if (sym.is_defined) {
                // adrp {xd}, {name}
                // add {xd}, {xd}, :lo12:{name}
                asm_write_global_addr(ctx.builder, xd, sym.name);
            } else {
                // adrp {xd}, :got:{name}
                // ldr {xd}, [{xd}, :got_lo12:{name}]
                asm_write_got_global_addr(ctx.builder, xd, sym.name);
            }
        }
        case HirExpr_Temp: {
            var e = e as *HirTempExpr;
            var temp = e.temp;
            write_slot_addr(ctx, temp.type, temp.slot_id, xd);
            asm_add_comment_to_last(ctx.builder, "temp @ %d", temp.slot_id);
        }
        case HirExpr_Member: {
            var e = e as *HirMemberExpr;
            var field = e.field;

            assert(e.left.type.kind == Type_Record, "asm_lower_expr_addr: left operand should be a record.");
            var sym = (e.left.type as *RecordType).sym;
            var field_index = find_record_field_by_name(sym, field.name);
            assert(field_index != -1, "asm_lower_expr_addr: field should exist in the record.");
            var field_offset = field_offset(sym, field_index);

            asm_lower_expr_addr(ctx, e.left, xd);
            // add {xd}, {xd}, #{field_offset}
            asm_write_add_ri(ctx.builder, 8, xd, xd, field_offset);
            asm_add_comment_to_last(ctx.builder, "member '%s'", field.name);
        }
        case HirExpr_Deref: {
            var e = e as *HirDerefExpr;
            asm_lower_expr(ctx, e.expr, xd);
        }
        case HirExpr_Index: {
            var e = e as *HirIndexExpr;
            var x1 = next_reg_except_1(ctx, xd);
            var x2 = next_reg_except_2(ctx, xd, x1);
            match (e.indexee.type.kind) {
                case Type_Ptr: {
                    // xd <- ...
                    asm_lower_expr(ctx, e.indexee, xd);
                    // x1 <- ...
                    asm_lower_expr(ctx, e.index, x1);
                }
                case Type_Arr: {
                    // xd <- ...
                    asm_lower_expr_addr(ctx, e.indexee, xd);
                    // x1 <- ...
                    asm_lower_expr(ctx, e.index, x1);
                }
                case _: {
                    unreachable("asm_lower_expr_addr: indexee should be a pointer or an array.");
                }
            }
            var elem_size = type_size(e.type);
            asm_write_mov_i(ctx.builder, 8, x2, elem_size);
            asm_write_binary_op_rrr(ctx.builder, "madd", xd, x1, x2, xd);
            free_reg(ctx, x1);
        }
        case other @ _: {
            unreachable_enum_case("asm_lower_expr_addr", other);
        }
    }
    lock_reg(ctx, xd);
}

func asm_lower_seq_expr(ctx: *mut CodegenCtx, e: *HirSeqExpr, xd: Reg) {
    var first = e.first;
    var second = e.second;
    // x1 <- ...
    var x1 = next_reg_except_1(ctx, xd);
    asm_lower_expr(ctx, first, x1);
    free_reg(ctx, x1);
    // xd <- ...
    asm_lower_expr(ctx, second, xd);
}

func asm_lower_int_expr(ctx: *mut CodegenCtx, e: *HirIntExpr, xd: Reg) {
    var int_val = e.value;
    // mov {xd}, #{int_val}
    asm_write_mov_i(ctx.builder, 8, xd, int_val);
}

func asm_lower_str_expr(ctx: *mut CodegenCtx, e: *HirStrExpr, xd: Reg) {
    var string_index = define_global_string(ctx, e.value, xd);
    asm_write_string_addr(ctx.builder, xd, string_index);
}

func asm_lower_cond_expr(ctx: *mut CodegenCtx, e: *HirCondExpr, xd: Reg) {
    var cond = e.cond;
    var then_expr = e.then_expr;
    var else_expr = e.else_expr;

    var if_label = next_label(ctx);
    var then_label = next_label(ctx);
    var else_label = next_label(ctx);
    var end_label = next_label(ctx);

    // L.if:
    asm_write_label(ctx.builder, if_label, "if");
    // x0 <- ...
    asm_lower_expr(ctx, cond, xd);
    // cbz x0, L.else
    asm_write_cbz(ctx.builder, xd, else_label, "else");
    unlock_reg(ctx, xd);

    // L.then:
    asm_write_label(ctx.builder, then_label, "then");
    // {xd} <- ...
    asm_lower_expr(ctx, then_expr, xd);
    // b L.end
    asm_write_b(ctx.builder, end_label, "end");

    // L.else:
    unlock_reg(ctx, xd);
    asm_write_label(ctx.builder, else_label, "else");
    // {xd} <- ...
    asm_lower_expr(ctx, else_expr, xd);

    // L.end:
    asm_write_label(ctx.builder, end_label, "end");
}

func asm_lower_loop_expr(ctx: *mut CodegenCtx, e: *HirLoopExpr, xd: Reg) {
    var cond = e.cond;
    var body = e.body;
    var step = e.step;

    var while_label = next_label(ctx);
    var do_label = next_label(ctx);
    var step_label = next_label(ctx);
    var done_label = next_label(ctx);

    var outer_loop = ctx.current_loop;
    ctx.current_loop = LoopCtx { step_label, done_label };

    // L.while:
    asm_write_label(ctx.builder, while_label, "while");
    // x0 <- ...
    asm_lower_expr(ctx, cond, xd);
    // cbz x0, L.done
    asm_write_cbz(ctx.builder, xd, done_label, "done");
    unlock_reg(ctx, xd);

    // L.do:
    asm_write_label(ctx.builder, do_label, "do");
    // x0 <- ...
    asm_lower_expr(ctx, body, xd);
    unlock_reg(ctx, xd);

    // L.step:
    asm_write_label(ctx.builder, step_label, "step");
    // x0 <- ...
    asm_lower_expr(ctx, step, xd);
    unlock_reg(ctx, xd);
    // b L.while
    asm_write_b(ctx.builder, while_label, "while");

    // L.done:
    asm_write_label(ctx.builder, done_label, "done");

    ctx.current_loop = outer_loop;
}

func asm_lower_jump_expr(ctx: *mut CodegenCtx, e: *HirJumpExpr, xd: Reg) {
    var is_break = e.is_break;
    if (is_break) {
        // b .L.done
        asm_write_b(ctx.builder, ctx.current_loop.done_label, "done");
    } else {
        // b .L.step
        asm_write_b(ctx.builder, ctx.current_loop.step_label, "step");
    }
}

func asm_lower_return_expr(ctx: *mut CodegenCtx, e: *HirReturnExpr, xd: Reg) {
    if (e.expr) {
        var ret_loc = &ctx.current_call_layout.ret_loc;

        if (ret_loc.is_indirect) {
            assert(hir_is_lvalue(e.expr), "asm_lower_return_expr: composite return should be an lvalue.");
            // x1 <- &...
            // ldr x0, [fp, #-8] // restore indirect result location
            // mov x2, #{type size}
            // bl memcpy

            spill_before_call(ctx);

            asm_lower_expr_addr(ctx, e.expr, Reg_X1);
            asm_write_ldrs_ri(ctx.builder, 8, Reg_X0, FP, -8);
            asm_add_comment_to_last(ctx.builder, "restore indirect result location");
            asm_write_mov_i(ctx.builder, 8, Reg_X2, type_size(e.expr.type));
            asm_write_bl(ctx.builder, "memcpy");

            unspill_after_call(ctx);
        } else if (ret_loc.is_reg) {
            if (is_scalar(e.expr.type)) {
                // x0 <- ...

                reserve_reg(ctx, Reg_X0);
                asm_lower_expr(ctx, e.expr, Reg_X0);
                free_reg(ctx, Reg_X0);
            } else {
                assert(hir_is_lvalue(e.expr), "asm_lower_return_expr: composite return should be an lvalue.");
                // x0 <- &...
                // ldr xn, [x0, #{n * 8}]
                // ...
                // ldr x0, [x0, #{0 * 8}]

                reserve_reg(ctx, Reg_X0);
                asm_lower_expr_addr(ctx, e.expr, Reg_X0);
                for (var i = ret_loc.n_regs - 1; i >= 0; i -= 1) {
                    var reg = (Reg_X0 as Int + i) as Reg;
                    asm_write_ldrs_ri(ctx.builder, 8, reg, Reg_X0, offset: i * 8);
                }
                free_reg(ctx, Reg_X0);
            }
        } else {
            unreachable("asm_lower_return_expr: invalid return location.");
        }
    }

    // b .L.ret
    asm_write_b(ctx.builder, ctx.ret_label, "ret");
}

func asm_lower_binary_op_expr(ctx: *mut CodegenCtx, e: *HirBinaryOpExpr, xd: Reg) {
    var op = e.op;
    var e1 = e.left;
    var e2 = e.right;
    var type = e.type;
    match (op) {
        case HirOp_Or: {
            asm_lower_expr_binary(ctx, "orr", e1, e2, xd);
        }
        case HirOp_Xor: {
            asm_lower_expr_binary(ctx, "eor", e1, e2, xd);
        }
        case HirOp_And: {
            asm_lower_expr_binary(ctx, "and", e1, e2, xd);
        }
        case HirOp_Shl: {
            asm_lower_expr_binary(ctx, "lsl", e1, e2, xd);
        }
        case HirOp_Shr: {
            asm_lower_expr_binary(ctx, "lsr", e1, e2, xd);
        }
        case HirOp_Eq: {
            asm_lower_expr_cmp(ctx, "eq", e1, e2, xd);
        }
        case HirOp_Ne: {
            asm_lower_expr_cmp(ctx, "ne", e1, e2, xd);
        }
        case HirOp_Lt: {
            asm_lower_expr_cmp(ctx, "lt", e1, e2, xd);
        }
        case HirOp_Le: {
            asm_lower_expr_cmp(ctx, "le", e1, e2, xd);
        }
        case HirOp_Gt: {
            asm_lower_expr_cmp(ctx, "gt", e1, e2, xd);
        }
        case HirOp_Ge: {
            asm_lower_expr_cmp(ctx, "ge", e1, e2, xd);
        }
        case HirOp_Add: {
            asm_lower_expr_binary(ctx, "add", e1, e2, xd);
        }
        case HirOp_Sub: {
            asm_lower_expr_binary(ctx, "sub", e1, e2, xd);
        }
        case HirOp_Mul: {
            asm_lower_expr_binary(ctx, "mul", e1, e2, xd);
        }
        case HirOp_Div: {
            asm_lower_expr_binary(ctx, "sdiv", e1, e2, xd);
        }
        case HirOp_Rem: {
            var x1 = next_reg_except_1(ctx, xd);
            var x2 = next_reg_except_2(ctx, xd, x1);
            // x1 <- ...
            asm_lower_expr(ctx, e1, x1);
            // x2 <- ...
            asm_lower_expr(ctx, e2, x2);
            // sdiv x0, x1, x2
            asm_write_binary_op_rr(ctx.builder, "sdiv", xd, x1, x2);
            // msub {xd}, x0, x2, x1
            asm_write_binary_op_rrr(ctx.builder, "msub", xd, xd, x2, x1);

            free_reg(ctx, x2);
            free_reg(ctx, x1);
        }
        case _: {
            unreachable("asm_lower_binary_op_expr");
        }
    }
}

func asm_lower_call_expr(ctx: *mut CodegenCtx, e: *HirCallExpr, xd: Reg, is_composite_assign: Bool) {
    var layout: CallLayout;
    layout = get_call_layout_for_call(e);

    var is_reg_ret_scalar = layout.ret_loc.is_reg && is_scalar(e.type);
    var is_reg_ret_composite = layout.ret_loc.is_reg && is_composite(e.type);
    var is_indirect_ret = layout.ret_loc.is_indirect;

    asm_write_comment(ctx.builder, "prepare call to %s", e.callee.name);

    spill_before_call(ctx);

    if (is_indirect_ret) {
        assert(is_composite_assign, "asm_lower_call_expr: destination should be address.");

        // Prepare the indirect return location
        // mov x8 xd
        asm_write_mov_r(ctx.builder, 8, Reg_X8, xd);
        lock_reg(ctx, Reg_X8);
    } else if (is_reg_ret_composite) {
        assert(is_composite_assign, "asm_lower_call_expr: destination should be address.");

        // Save destination for later
        write_push(ctx, xd);
    }

    for (var i = 0; i < layout.n_args; i += 1) {
        var arg = e.args[i];
        var arg_loc = &layout.arg_locs[i];
        match (arg_loc.kind) {
            case ArgLocation_Reg if is_scalar(arg.type): {
                var arg_loc = &arg_loc.Reg;
                // Pass scalar in register.
                // x{i} <- ...
                asm_lower_expr(ctx, arg, arg_loc.reg as Reg);
            }
            case ArgLocation_Reg if is_pass_by_ptr(arg.type) && hir_is_lvalue(arg): {
                var arg_loc = &arg_loc.Reg;
                // Pass composite by pointer in register.
                // x{i} <- &...
                asm_lower_expr_addr(ctx, arg, arg_loc.reg as Reg);
            }
            case ArgLocation_Reg if hir_is_lvalue(arg): {
                var arg_loc = &arg_loc.Reg;
                // Pass 8 or 16 byte aggregate in consecutive registers.
                //
                // x{lo} <- &...
                // ldr x{lo+n} [x{lo}, #8 * n]
                // ...
                // ldr x{lo+0} [x{lo}, #8 * 0]
                var lo = arg_loc.reg;
                var n = arg_loc.n_regs;
                asm_lower_expr_addr(ctx, arg, lo as Reg);
                for (var j = n - 1; j >= 0; j -= 1) {
                    asm_write_ldrs_ri(ctx.builder, 8, (lo + j) as Reg, lo as Reg, 8 * j);
                }
            }
            case ArgLocation_Stack if is_scalar(arg.type): {
                var arg_loc = &arg_loc.Stack;
                // Pass scalar on stack.
                // {x1} <- ...
                // str {x1}, [sp, #{offset}]
                var x1 = next_reg_except_1(ctx, SP);
                asm_lower_expr(ctx, arg, x1);
                asm_write_str_ri(ctx.builder, 8, x1, SP, arg_loc.offset);
                free_reg(ctx, x1);
            }
            case ArgLocation_Stack if hir_is_lvalue(arg): {
                var arg_loc = &arg_loc.Stack;
                // Pass aggregate on stack.
                //
                // x1 <- &...
                // add x0, sp, #{offset}
                // mov x2, #{size}
                // bl memcpy
                spill_before_call(ctx);
                asm_lower_expr_addr(ctx, arg, Reg_X1);
                asm_write_add_ri(ctx.builder, 8, Reg_X0, SP, arg_loc.offset);
                asm_write_mov_i(ctx.builder, 8, Reg_X2, type_size(arg.type));
                asm_write_bl(ctx.builder, "memcpy");
                unspill_after_call(ctx);
            }
            case _: {
                unreachable("asm_lower_call_expr");
            }
        }
    }

    // bl {name}
    asm_write_bl(ctx.builder, e.callee.name);

    var xm = layout.ret_loc.n_regs as Reg;

    if (is_reg_ret_composite) {
        // Restore destination
        write_pop(ctx, xm);
    }

    if (is_reg_ret_scalar) {
        write_sign_extend(ctx, e.type, xd, Reg_X0);
    } else if (is_reg_ret_composite) {
        // xm <- &...
        // str x0, [xm]
        // ...
        // str xn, [xm, #8 * n]
        var n_regs = layout.ret_loc.n_regs;
        for (var i = 0; i < n_regs; i += 1) {
            var reg = (Reg_X0 as Int + i) as Reg;
            asm_write_str_ri(ctx.builder, 8, reg, xm, 8 * i);
        }
    } else if (layout.ret_loc.is_reg) {
        assert(false, "asm_lower_call_expr: Only scalar and composite types can be returned in registers.");
    } else if (is_indirect_ret) {
        // Nothing to do for the caller
    }

    unspill_after_call(ctx);

    asm_write_comment(ctx.builder, "call to %s done", e.callee.name);

    ctx.max_arg_pass_size = int_max(ctx.max_arg_pass_size, layout.stack_space);

    call_layout_drop(&layout);
}

func asm_lower_addr_expr(ctx: *mut CodegenCtx, e: *HirAddrExpr, xd: Reg) {
    var subexpr = e.expr;
    asm_lower_expr_addr(ctx, subexpr, xd);
}

func asm_lower_assign_expr(ctx: *mut CodegenCtx, e: *HirAssignExpr, xd: Reg) {
    var dst = e.dst;
    var src = e.src;
    if (is_scalar(dst.type)) {
        var x1 = next_reg_except_1(ctx, xd);
        var x2 = next_reg_except_2(ctx, xd, x1);

        // x1 <- ...
        asm_lower_expr_addr(ctx, dst, x1);
        // x2 <- ...
        asm_lower_expr(ctx, src, x2);
        // strx x2, [x1]
        asm_write_str_r(ctx.builder, type_size(dst.type), x2, x1);

        free_reg(ctx, x2);
        free_reg(ctx, x1);
    } else if (is_composite(dst.type) && src.kind == HirExpr_Call) {
        asm_lower_expr_addr(ctx, dst, xd);
        unlock_reg(ctx, xd);
        asm_lower_call_expr(ctx, src as *HirCallExpr, xd, is_composite_assign: true);
    } else if (is_composite(dst.type)) {
        spill_before_call(ctx);

        // x0, x1 <- ...
        asm_lower_expr_addr(ctx, dst, Reg_X0);
        asm_lower_expr_addr(ctx, src, Reg_X1);
        // mov x2, #{size}
        asm_write_mov_i(ctx.builder, 8, Reg_X2, type_size(dst.type));
        // bl memcpy
        asm_write_bl(ctx.builder, "memcpy");

        unspill_after_call(ctx);
    } else {
        unreachable("asm_lower_assign_expr");
    }
}

func asm_lower_cast_expr(ctx: *mut CodegenCtx, e: *HirCastExpr, xd: Reg) {
    var subexpr = e.expr;
    var target = e.type;
    var source = subexpr.type;
    asm_lower_expr(ctx, subexpr, xd);
    assert(is_scalar(target) && is_scalar(source), "asm_lower_expr: <cast> should have scalar types.");
    if (type_size(target) < type_size(source)) {
        write_sign_extend(ctx, target, xd, xd);
    } else {
        // no-op
    }
}

func asm_lower_expr(ctx: *mut CodegenCtx, e: *HirExpr, xd: Reg) {
    match (e.kind) {
        case _ if hir_is_lvalue(e): {
            assert(is_scalar(e.type), "asm_lower_expr: lvalue must evaluate to a scalar.");
            asm_lower_expr_addr(ctx, e, xd);
            // ldr {xd}, [{xd}]
            asm_write_ldrs_r(ctx.builder, type_size(e.type), xd, xd);
        }
        case HirExpr_Skip: {
            // nop
        }
        case HirExpr_Seq: {
            asm_lower_seq_expr(ctx, e as *HirSeqExpr, xd);
        }
        case HirExpr_Int: {
            asm_lower_int_expr(ctx, e as *HirIntExpr, xd);
        }
        case HirExpr_Str: {
            asm_lower_str_expr(ctx, e as *HirStrExpr, xd);
        }
        case HirExpr_Cond: {
            asm_lower_cond_expr(ctx, e as *HirCondExpr, xd);
        }
        case HirExpr_Loop: {
            asm_lower_loop_expr(ctx, e as *HirLoopExpr, xd);
        }
        case HirExpr_Jump: {
            asm_lower_jump_expr(ctx, e as *HirJumpExpr, xd);
        }
        case HirExpr_Return: {
            asm_lower_return_expr(ctx, e as *HirReturnExpr, xd);
        }
        case HirExpr_BinaryOp: {
            asm_lower_binary_op_expr(ctx, e as *HirBinaryOpExpr, xd);
        }
        case HirExpr_Call: {
            asm_lower_call_expr(ctx, e as *HirCallExpr, xd, is_composite_assign: false);
        }
        case HirExpr_Addr: {
            asm_lower_addr_expr(ctx, e as *HirAddrExpr, xd);
        }
        case HirExpr_Assign: {
            asm_lower_assign_expr(ctx, e as *HirAssignExpr, xd);
        }
        case HirExpr_Cast: {
            asm_lower_cast_expr(ctx, e as *HirCastExpr, xd);
        }
        case HirExpr_Unreachable: {
            asm_write_comment(ctx.builder, "<- unreachable");
        }
        case _: {
            unreachable("asm_lower_expr");
        }
    }
    lock_reg(ctx, xd);
}

func asm_lower_func(ctx: *mut CodegenCtx, sym: *FuncSym, hir_body: *HirExpr) {
    var layout = &ctx.current_call_layout;

    // Save indirect result location
    if (layout.ret_loc.is_indirect) {
        // str x8, [fp, #-8]
        asm_write_str_ri(ctx.builder, 8, Reg_X8, FP, -8);
        asm_add_comment_to_last(ctx.builder, "save indirect result location");
    }

    // Save varargs
    if (sym.is_variadic) {
        var gr_base = ctx.saved_varargs_gr_top_fp_offset - ctx.saved_varargs_gr_size;
        for (var i = layout.next_gpr; i < 8; i += 1) {
            var src_reg = i as Reg;
            var fp_offset = gr_base + 8 * (i - layout.next_gpr);
            // str x{i}, [fp, #{fp_offset}]
            asm_write_str_ri(ctx.builder, 8, src_reg, FP, fp_offset);
        }
    }

    // Copy arguments to stack slots
    for (var i = 0; i < layout.n_args; i += 1) {
        var param = list_get(sym.params, i) as *FuncParam;
        var local = list_get(sym.locals, i) as *mut LocalSym;
        var arg_loc = &layout.arg_locs[i];
        if (arg_loc.kind == ArgLocation_Reg) {
            var arg_loc = &arg_loc.Reg;
            for (var j = 0; j < arg_loc.n_regs; j += 1) {
                var reg = (arg_loc.reg + j) as Reg;
                write_slot_store_partial(ctx, param.type, local.slot_id, 8 * j, reg);
            }
            local.is_indirect = is_pass_by_ptr(param.type);
        } else if (type_size(param.type) <= 8) {
            var arg_loc = &arg_loc.Stack;
            // ldr x0, [fp, #{spills_size + arg_offset}]
            asm_write_ldrs_r_fp_from_frame_start(ctx.builder, 8, Reg_X0, arg_loc.offset);
            // str x0, [fp, #{fp_offset}]
            write_slot_store(ctx, param.type, local.slot_id, Reg_X0);
        } else {
            // Passed on stack. Must be copied to local slot. Handled separately below
            // to make sure calling memcpy doesn't clobber the remaining unread arguments.
        }
    }

    // Create va_list if needed
    if (sym.rest_param_name) {
        var local = list_get(sym.locals, layout.n_args) as *LocalSym;
        var slot_id = local.slot_id;
        var fp_offset = get_fp_offset_for_slot(ctx, mk_rest_param_type(), slot_id);

        // va_list layout:
        //
        // struct va_list {
        //     stack: *Void,
        //     gr_top: *Void,
        //     vr_top: *Void,
        //     gr_offs: Int32,
        //     vr_offs: Int32,
        // }

        asm_write_comment(ctx.builder, "initialize va_list");

        // x0 <- &local
        write_slot_addr(ctx, mk_rest_param_type(), slot_id, Reg_X0);
        // ; va_list.stack
        // add x1, FP, #{spills_size}
        // str x1, [x0, #0]
        asm_write_add_r_fp_from_frame_start(ctx.builder, 8, Reg_X1, 0);
        asm_write_str_ri(ctx.builder, 8, Reg_X1, Reg_X0, 0);
        // ; va_list.gr_top
        // add x1, FP, #{saved_varargs_gr_top_fp_offset}
        // str x1, [x0, #8]
        asm_write_add_ri(ctx.builder, 8, Reg_X1, FP, ctx.saved_varargs_gr_top_fp_offset);
        asm_write_str_ri(ctx.builder, 8, Reg_X1, Reg_X0, 8);
        // ; va_list.vr_top
        // add x1, FP, #{saved_varargs_gr_top_fp_offset - saved_varargs_gr_size}
        // str x1, [x0, #16]
        asm_write_add_ri(ctx.builder, 8, Reg_X1, FP, ctx.saved_varargs_gr_top_fp_offset - ctx.saved_varargs_gr_size);
        asm_write_str_ri(ctx.builder, 8, Reg_X1, Reg_X0, 16);
        // ; va_list.gr_offs
        // mov x1, #{-saved_gp_varargs_size}
        // str x1, [x0, #24]
        asm_write_mov_i(ctx.builder, 4, Reg_X1, -ctx.saved_varargs_gr_size);
        asm_write_str_ri(ctx.builder, 4, Reg_X1, Reg_X0, 24);
        // ; va_list.vr_offs
        // mov x1, #0
        // str x1, [x0, #28]
        asm_write_mov_i(ctx.builder, 4, Reg_X1, 0);
        asm_write_str_ri(ctx.builder, 4, Reg_X1, Reg_X0, 28);

        asm_write_comment(ctx.builder, "va_list initialized");
    }

    for (var i = 0; i < layout.n_args; i += 1) {
        var param = list_get(sym.params, i) as *FuncParam;
        var local = list_get(sym.locals, i) as *LocalSym;
        var arg_loc = &layout.arg_locs[i];

        if (arg_loc.kind == ArgLocation_Reg) {
            // Already handled
        } else if (type_size(param.type) <= 8) {
            // Already handled
        } else {
            var arg_loc = &arg_loc.Stack;
            // add x0, fp, #{??}
            // add x1, fp, #{spills_size + arg_offset}
            // mov x2, #{size}
            // bl memcpy

            write_slot_addr(ctx, param.type, local.slot_id, Reg_X0);
            asm_write_add_r_fp_from_frame_start(ctx.builder, 8, Reg_X1, arg_loc.offset);
            asm_write_mov_i(ctx.builder, 8, Reg_X2, type_size(param.type));
            asm_write_bl(ctx.builder, "memcpy");
        }
    }

    asm_lower_expr(ctx, hir_body, Reg_X0);

    // L.ret:
    asm_write_label(ctx.builder, ctx.ret_label, "ret");
}

func layout_slots(ctx: *mut CodegenCtx, start_fp_offset: Int): Int {
    var sym = ctx.current_func;

    var n_locals = list_len(sym.locals);
    var n_temps = list_len(sym.temps);
    var n_slots = n_locals + n_temps;
    var slots = calloc(n_slots, sizeof(Slot)) as *mut Slot;

    var next_fp_offset = start_fp_offset;
    for (var i = 0; i < n_locals; i += 1) {
        var local = list_get(sym.locals, i) as *LocalSym;
        next_fp_offset = -align_up(-next_fp_offset + type_size(local.type), type_align(local.type));
        slots[i] = Slot { fp_offset: next_fp_offset };
    }
    for (var i = 0; i < n_temps; i += 1) {
        var temp = list_get(sym.temps, i) as *HirTemp;
        next_fp_offset = -align_up(-next_fp_offset + type_size(temp.type), type_align(temp.type));
        slots[n_locals + i] = Slot { fp_offset: next_fp_offset };
    }

    ctx.n_slots = n_slots;
    ctx.slots = slots;

    return start_fp_offset - next_fp_offset;
}

func emit_func(ctx: *mut CodegenCtx, sym: *mut FuncSym) {
    var hir_body = hir_lower(sym, sym.body as *Stmt);

    ctx.current_func = sym;
    ctx.ret_label = next_label(ctx);
    ctx.current_call_layout = get_call_layout_for_func(sym);

    var next_fp_offset = 0;

    if (ctx.current_call_layout.ret_loc.is_indirect) {
        next_fp_offset -= 8;
    }

    if (sym.is_variadic && ctx.current_call_layout.next_gpr < 8) {
        ctx.saved_varargs_gr_top_fp_offset = next_fp_offset;
        var n_unallocated_gprs = 8 - ctx.current_call_layout.next_gpr;
        next_fp_offset -= 8 * n_unallocated_gprs;
        ctx.saved_varargs_gr_size = 8 * n_unallocated_gprs;
    }

    var slots_size = layout_slots(ctx, next_fp_offset);
    next_fp_offset -= slots_size;

    ctx.scratch_fp_offset = -align_up(-next_fp_offset, 8);
    next_fp_offset = ctx.scratch_fp_offset;

    ctx.builder = asm_builder_new();

    memset(&ctx.regs, 0, sizeof([RegState; N_REGS]));
    for (var i = 0; i < N_REGS; i += 1) {
        ctx.regs[i].reservations = list_new();
    }

    asm_write_comment(ctx.builder, "body");
    asm_lower_func(ctx, sym, hir_body);

    var spills: [Reg; N_REGS];

    spills[0] = Reg_X29;
    spills[1] = Reg_X30;
    var n_spills = 2; // x29, x30

    for (var i = 0; i < N_REGS; i += 1) {
        var is_reg_used = ctx.regs[i].n_writes > 0;
        if (is_reg_used && reg_is_callee_saved(i as Reg)) {
            spills[n_spills] = i as Reg;
            n_spills += 1;
        }
    }

    var spills_size = align_up(n_spills * 8, 16);

    var frame_size = align_up(spills_size + -next_fp_offset + ctx.max_scratch_size + ctx.max_arg_pass_size, 16);

    asm_print_func(ctx.out, &AsmFunc {
        name: sym.name,
        builder: ctx.builder,
        frame_size: frame_size,
        spills: spills,
        n_spills: n_spills,
        spills_size: spills_size,
    });

    call_layout_drop(&ctx.current_call_layout);
}

func emit_global(ctx: *mut CodegenCtx, sym: *GlobalSym) {
    asm_print_global(
        ctx.out,
        sym.name,
        type_size(sym.type),
        type_align(sym.type)
    );
}

func emit_sym(ctx: *mut CodegenCtx, sym: *mut Sym) {
    if (!sym.is_defined) {
        return;
    }
    match (sym.kind) {
        case Sym_Func: {
            emit_func(ctx, sym as *mut FuncSym);
        }
        case Sym_Global: {
            emit_global(ctx, sym as *GlobalSym);
        }
    }
}

func emit_program(out: *mut File, syms: *List) {
    var ctx = calloc(1, sizeof(CodegenCtx)) as *mut CodegenCtx;
    ctx.out = out;
    ctx.strings = list_new();

    for (var i = 0; i < list_len(syms); i += 1) {
        var sym = list_get(syms, i) as *mut Sym;
        emit_sym(ctx, sym);
    }

    var n_strings = list_len(ctx.strings);
    for (var i = 0; i < n_strings; i += 1) {
        var string = list_get(ctx.strings, i) as *StringBuffer;
        asm_print_string(ctx.out, i, string);
    }
}
