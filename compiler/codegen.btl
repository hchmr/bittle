include "bittle.btls";

/// Note: [Frame layout]
/// ~~~~~~~~~~~~~~~~~~~~
/// The code generator organizes the stack frame as follows:
/// ```
/// +-------------------+
/// |                   |
/// | spills            |
/// |                   |
/// +-------------------+ <- FP
/// | saved x8          |
/// +-------------------+
/// |                   |
/// | saved varargs     |
/// |                   |
/// +-------------------+
/// |                   |
/// | slots             |
/// |                   |
/// +-------------------+
/// |                   |
/// | scratch           |
/// |                   |
/// +-------------------+
/// |                   |
/// | outgoing args     |
/// |                   |
/// +-------------------+ <- SP
/// ```

//==============================================================================
//== Helper Functions

func is_int_or_enum(t: *Type, max_size: Int): Bool {
    if (t.kind == Type_Int) {
        return (t as *IntType).size <= max_size;
    } else if (t.kind == Type_Enum) {
        return (t as *EnumType).sym.size <= max_size;
    } else {
        return false;
    }
}

func reg_width_from_size(size: Int): Int {
    return size < 8 ? 4 : 8;
}

//==============================================================================
//== Assembly DSL

enum Reg {
    Reg_X0, Reg_X1, Reg_X2, Reg_X3, Reg_X4, Reg_X5, Reg_X6, Reg_X7, Reg_X8,
    Reg_X9, Reg_X10, Reg_X11, Reg_X12, Reg_X13, Reg_X14, Reg_X15,
    Reg_X16, Reg_X17, Reg_X18, Reg_X19, Reg_X20, Reg_X21, Reg_X22, Reg_X23, Reg_X24,
    Reg_X25, Reg_X26, Reg_X27, Reg_X28, Reg_X29, Reg_X30, Reg_X31,
    FP = Reg_X29, LR = Reg_X30, SP = Reg_X31,
}

const N_REGS = 32;
const N_CALLEE_SAVED_REGS = Reg_X28 - Reg_X19 + 1;

func reg_is_callee_saved(reg: Reg): Bool {
    return Reg_X19 <= reg && reg <= Reg_X28;
}

enum AsmOperandKind {
    AsmOperand_Reg,
    AsmOperand_Mem,
    AsmOperand_Int,
    AsmOperand_Label,
    AsmOperand_Global,
    AsmOperand_FpFromFrameStart,
    AsmOperand_SpFromFp,
    AsmOperand_Raw,
}

struct AsmOperand {
    kind: AsmOperandKind,
}

// e.g. x0
struct AsmRegOperand : AsmOperand {
    width: Int,
    reg: Reg,
    shift: Int,
}

// e.g. [x0]
// e.g. [x0, #42]
struct AsmMemOperand : AsmOperand {
    n_args: Int,
    args: **AsmOperand,
}

// e.g. #42
// e.g. #1, lsl 16
struct AsmIntOperand : AsmOperand {
    value: Int,
    shift: Int,
}

// e.g. .L0
struct AsmLabelOperand : AsmOperand {
    counter: Int, // Only used for auto-generated labels
    suffix: *Char,
}

// e.g. :got:my_global
// e.g. str3
struct AsmGlobalOperand : AsmOperand {
    relocation_spec: *Char, // :lo12, :got, :got_lo12
    prefix: *Char,
    counter: Int, // Only used for strings
}

struct AsmFpFromFrameStartOperand : AsmOperand {
    offset: Int,
}

struct AsmSpFromFpOperand : AsmOperand {
    fp_offset: Int,
}

struct AsmRawOperand : AsmOperand {
    op: *Char,
}

struct AsmInstr {
    op: *Char,
    n_args: Int,
    args: **AsmOperand,
    comment: *Char,
}

struct AsmFunc {
    name: *Char,
    instrs: *List, // List<*AsmInstr>
    frame_size: Int,
    n_spills: Int,
    spills: [Reg; N_REGS],
    spills_size: Int,
}

func asm_mk_reg_operand(width: Int, reg: Reg, shift: Int): *AsmRegOperand {
    var operand: *AsmRegOperand = calloc(1, sizeof(AsmRegOperand));
    *operand = AsmRegOperand {
        kind: AsmOperand_Reg,
        width: width,
        reg: reg,
        shift: shift,
    };
    return operand;
}

func asm_mk_int_operand(value: Int): *AsmIntOperand {
    var operand: *AsmIntOperand = calloc(1, sizeof(AsmIntOperand));
    *operand = AsmIntOperand {
        kind: AsmOperand_Int,
        value: value,
        shift: 0,
    };
    return operand;
}

func asm_mk_label_operand(counter: Int, suffix: *Char): *AsmLabelOperand {
    var operand: *AsmLabelOperand = calloc(1, sizeof(AsmLabelOperand));
    *operand = AsmLabelOperand {
        kind: AsmOperand_Label,
        counter: counter,
        suffix: suffix,
    };
    return operand;
}

func asm_mk_global_operand(relocation_spec: *Char, prefix: *Char, suffix: Int): *AsmGlobalOperand {
    var operand: *AsmGlobalOperand = calloc(1, sizeof(AsmGlobalOperand));
    *operand = AsmGlobalOperand {
        kind: AsmOperand_Global,
        relocation_spec: relocation_spec,
        prefix: prefix,
        counter: suffix,
    };
    return operand;
}

func asm_mk_fp_from_frame_start_operand(offset: Int): *AsmFpFromFrameStartOperand {
    var operand: *AsmFpFromFrameStartOperand = calloc(1, sizeof(AsmFpFromFrameStartOperand));
    *operand = AsmFpFromFrameStartOperand {
        kind: AsmOperand_FpFromFrameStart,
        offset: offset,
    };
    return operand;
}

func asm_mk_sp_from_fp_operand(fp_offset: Int): *AsmSpFromFpOperand {
    var operand: *AsmSpFromFpOperand = calloc(1, sizeof(AsmSpFromFpOperand));
    *operand = AsmSpFromFpOperand {
        kind: AsmOperand_SpFromFp,
        fp_offset: fp_offset,
    };
    return operand;
}

func asm_mk_raw_operand(op: *Char): *AsmRawOperand {
    var operand: *AsmRawOperand = calloc(1, sizeof(AsmRawOperand));
    *operand = AsmRawOperand {
        kind: AsmOperand_Raw,
        op: op,
    };
    return operand;
}

func asm_mk_mem_operand(n_args: Int): *AsmMemOperand {
    var operand: *AsmMemOperand = calloc(1, sizeof(AsmMemOperand));
    *operand = AsmMemOperand {
        kind: AsmOperand_Mem,
        n_args: n_args,
        args: calloc(n_args, sizeof(*AsmOperand)),
    };
    return operand;
}

func asm_mk_reg_mem_operand(reg: Reg, offset: Int): *AsmMemOperand {
    var operand = asm_mk_mem_operand(offset != 0 ? 2 : 1);
    operand.args[0] = asm_mk_reg_operand(8, reg, shift: 0);
    if (offset != 0) {
        operand.args[1] = asm_mk_int_operand(offset);
    }
    return operand;
}

func asm_mk_fp_from_frame_start_mem_operand(offset: Int): *AsmMemOperand {
    var operand = asm_mk_mem_operand(2);
    operand.args[0] = asm_mk_reg_operand(8, FP, shift: 0);
    operand.args[1] = asm_mk_fp_from_frame_start_operand(offset);
    return operand;
}

func asm_mk_sp_from_fp_mem_operand(fp_offset: Int): *AsmMemOperand {
    var operand = asm_mk_mem_operand(2);
    operand.args[0] = asm_mk_reg_operand(8, SP, shift: 0);
    operand.args[1] = asm_mk_sp_from_fp_operand(fp_offset);
    return operand;
}

func asm_mk_instr(op: *Char, n_args: Int): *AsmInstr {
    var instr: *AsmInstr = calloc(1, sizeof(AsmInstr));
    *instr = AsmInstr {
        op: op,
        n_args: n_args,
        args: calloc(n_args, sizeof(*AsmOperand)),
        comment: null,
    };
    return instr;
}

//==============================================================================
//== Instruction building

func asm_write_instr(builder: *List, op: *Char, n_args: Int): *AsmInstr {
    var instr = asm_mk_instr(op, n_args);
    list_push(builder, instr);
    return instr;
}

// Data Movement

func asm_get_load_op(width: Int): *Char {
    return
        width == 1 ? "ldrsb" :
        width == 2 ? "ldrsh" :
        width == 4 ? "ldrsw" :
        "ldr";
}

func asm_get_store_op(width: Int): *Char {
    return
        width == 1 ? "strb" :
        width == 2 ? "strh" :
        "str";
}

func asm_write_mov_r(builder: *List, width: Int, xd: Reg, x1: Reg) {
    var instr = asm_write_instr(builder, "mov", n_args: 2);
    instr.args[0] = asm_mk_reg_operand(width, xd, shift: 0);
    instr.args[1] = asm_mk_reg_operand(width, x1, shift: 0);
}

func asm_write_mov_i(builder: *List, width: Int, xd: Reg, value: Int) {
    if (value == 0) {
        var instr = asm_write_instr(builder, "mov", n_args: 2);
        instr.args[0] = asm_mk_reg_operand(width, xd, shift: 0);
        instr.args[1] = asm_mk_int_operand(0);
        return;
    }

    var MASK_16 = (1 << 16) - 1;

    var n_gen = 0;
    for (var i = 0; i < 4; i += 1) {
        var lo16 = value & MASK_16;

        if (lo16 != 0 || i == 3 && n_gen == 0) {
            var op = n_gen == 0 ? "movz" : "movk";

            var instr = asm_write_instr(builder, op, n_args: 2);
            instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
            instr.args[1] = asm_mk_int_operand(lo16);
            (instr.args[1] as *AsmIntOperand).shift = i * 16;

            n_gen += 1;
        }

        value >>= 16;
    }

    assert(n_gen > 0, "asm_write_mov_i: should generate at least one instruction.");
}

func asm_write_ldrs(builder: *List, width: Int, dst: *AsmOperand, src: *AsmOperand) {
    var op = asm_get_load_op(width);
    var instr = asm_write_instr(builder, op, n_args: 2);
    instr.args[0] = dst;
    instr.args[1] = src;
}

func asm_write_ldrs_ri(builder: *List, width: Int, xd: Reg, xs: Reg, offset: Int) {
    var dst_operand = asm_mk_reg_operand(8, xd, shift: 0);
    var src_operand = asm_mk_reg_mem_operand(xs, offset);
    asm_write_ldrs(builder, width, dst_operand, src_operand);
}

func asm_write_ldrs_r(builder: *List, width: Int, xd: Reg, xs: Reg) {
    asm_write_ldrs_ri(builder, width, xd, xs, 0);
}

func asm_write_ldrs_r_fp_from_frame_start(builder: *List, width: Int, xd: Reg, offset: Int) {
    var dst_operand = asm_mk_reg_operand(8, xd, shift: 0);
    var src_operand = asm_mk_fp_from_frame_start_mem_operand(offset);
    asm_write_ldrs(builder, width, dst_operand, src_operand);
}

func asm_write_ldrs_r_sp_from_fp(builder: *List, width: Int, xd: Reg, fp_offset: Int) {
    var dst_operand = asm_mk_reg_operand(8, xd, shift: 0);
    var src_operand = asm_mk_sp_from_fp_mem_operand(fp_offset);
    asm_write_ldrs(builder, width, dst_operand, src_operand);
}

func asm_write_str(builder: *List, width: Int, src: *AsmOperand, dst: *AsmOperand) {
    var op = asm_get_store_op(width);
    var instr = asm_write_instr(builder, op, n_args: 2);
    instr.args[0] = src;
    instr.args[1] = dst;
}

func asm_write_str_ri(builder: *List, width: Int, xs: Reg, xd: Reg, offset: Int) {
    var src_width = reg_width_from_size(width);
    var src_operand = asm_mk_reg_operand(src_width, xs, shift: 0);
    var dst_operand = asm_mk_reg_mem_operand(xd, offset);
    asm_write_str(builder, width, src_operand, dst_operand);
}

func asm_write_str_r(builder: *List, width: Int, xs: Reg, xd: Reg) {
    asm_write_str_ri(builder, width, xs, xd, 0);
}

func asm_write_str_r_sp_from_fp(builder: *List, width: Int, xs: Reg, fp_offset: Int) {
    var src_width = reg_width_from_size(width);
    var src_operand = asm_mk_reg_operand(src_width, xs, shift: 0);
    var dst_operand = asm_mk_sp_from_fp_mem_operand(fp_offset);
    asm_write_str(builder, width, src_operand, dst_operand);
}

// Arithmetic, Logic and Comparison

// sxt {xd}, {x1}
func asm_write_sxt(builder: *List, dst_size: Int, src_size: Int, xd: Reg, x1: Reg) {
    var op =
        src_size == 1 ? "sxtb" :
        src_size == 2 ? "sxth" :
        "sxtw";

    var dst_width = reg_width_from_size(dst_size);
    var src_width = reg_width_from_size(src_size);

    var instr = asm_write_instr(builder, op, n_args: 2);
    instr.args[0] = asm_mk_reg_operand(dst_width, xd, shift: 0);
    instr.args[1] = asm_mk_reg_operand(src_width, x1, shift: 0);
}

func asm_write_binary_op_rr(builder: *List, op: *Char, xd: Reg, x1: Reg, x2: Reg) {
    var instr = asm_write_instr(builder, op, n_args: 3);
    instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    instr.args[1] = asm_mk_reg_operand(8, x1, shift: 0);
    instr.args[2] = asm_mk_reg_operand(8, x2, shift: 0);
}

func asm_write_binary_op_ri(builder: *List, op: *Char, xd: Reg, x1: Reg, imm: Int) {
    var instr = asm_write_instr(builder, op, n_args: 3);
    instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    instr.args[1] = asm_mk_reg_operand(8, x1, shift: 0);
    instr.args[2] = asm_mk_int_operand(imm);
}

func asm_write_binary_op_rrr(builder: *List, op: *Char, xd: Reg, x1: Reg, x2: Reg, x3: Reg) {
    var instr = asm_write_instr(builder, op, n_args: 4);
    instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    instr.args[1] = asm_mk_reg_operand(8, x1, shift: 0);
    instr.args[2] = asm_mk_reg_operand(8, x2, shift: 0);
    instr.args[3] = asm_mk_reg_operand(8, x3, shift: 0);
}

func asm_write_add_ri(builder: *List, width: Int, xd: Reg, x1: Reg, imm: Int) {
    asm_write_binary_op_ri(builder, "add", xd, x1, imm);
}

func asm_write_add_r_fp_from_frame_start(builder: *List, width: Int, xd: Reg, offset: Int) {
    var instr = asm_write_instr(builder, "add", n_args: 3);
    instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    instr.args[1] = asm_mk_reg_operand(8, FP, shift: 0);
    instr.args[2] = asm_mk_fp_from_frame_start_operand(offset);
}

func asm_write_add_rr(builder: *List, width: Int, xd: Reg, x1: Reg, x2: Reg) {
    asm_write_binary_op_rr(builder, "add", xd, x1, x2);
}

func asm_write_cmp_rr(builder: *List, width: Int, xd: Reg, x1: Reg) {
    var instr = asm_write_instr(builder, "cmp", n_args: 2);
    instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    instr.args[1] = asm_mk_reg_operand(8, x1, shift: 0);
}

func asm_write_cset(builder: *List, width: Int, xd: Reg, op: *Char) {
    var instr = asm_write_instr(builder, "cset", n_args: 2);
    instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    instr.args[1] = asm_mk_raw_operand(op);
}

// Control Flow

// cbz {xd}, {label}
func asm_write_cbz(builder: *List, xd: Reg, counter: Int, suffix: *Char) {
    var instr = asm_write_instr(builder, "cbz", n_args: 2);
    instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    instr.args[1] = asm_mk_label_operand(counter, suffix);
}

// b {label}
func asm_write_b(builder: *List, counter: Int, suffix: *Char) {
    var instr = asm_write_instr(builder, "b", n_args: 1);
    instr.args[0] = asm_mk_label_operand(counter, suffix);
}

// bl {name}
func asm_write_bl(builder: *List, name: *Char) {
    var instr = asm_write_instr(builder, "bl", n_args: 1);
    instr.args[0] = asm_mk_global_operand("", name, -1);
}

// Addressing

// addrp {xd}, {name}
// add {xd}, {xd}, :lo12:{name}
func asm_write_global_addr(builder: *List, xd: Reg, name: *Char) {
    var addrp_instr = asm_write_instr(builder, "adrp", n_args: 2);
    addrp_instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    addrp_instr.args[1] = asm_mk_global_operand("", name, -1);

    var instr = asm_write_instr(builder, "add", n_args: 3);
    instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    instr.args[1] = asm_mk_reg_operand(8, xd, shift: 0);
    instr.args[2] = asm_mk_global_operand(":lo12:", name, -1);
}

// addrp {xd}, :got:{name}
// ldr {xd}, [{xd}, :got_lo12:{name}]
func asm_write_got_global_addr(builder: *List, xd: Reg, name: *Char) {
    var addrp_instr = asm_write_instr(builder, "adrp", n_args: 2);
    addrp_instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    addrp_instr.args[1] = asm_mk_global_operand(":got:", name, -1);

    var mem_operand = asm_mk_mem_operand(2);
    mem_operand.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    mem_operand.args[1] = asm_mk_global_operand(":got_lo12:", name, -1);

    var ldr_instr = asm_write_instr(builder, "ldr", n_args: 2);
    ldr_instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    ldr_instr.args[1] = mem_operand;
}

// addrp {xd}, str{id}
// add {xd}, {xd}, :lo12:str{id}
func asm_write_string_addr(builder: *List, xd: Reg, id: Int) {
    var addrp_instr = asm_write_instr(builder, "adrp", n_args: 2);
    addrp_instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    addrp_instr.args[1] = asm_mk_global_operand("", ".L.str.", id);

    var add_instr = asm_write_instr(builder, "add", n_args: 3);
    add_instr.args[0] = asm_mk_reg_operand(8, xd, shift: 0);
    add_instr.args[1] = asm_mk_reg_operand(8, xd, shift: 0);
    add_instr.args[2] = asm_mk_global_operand(":lo12:", ".L.str.", id);
}

// Miscellaneous

func asm_write_label(builder: *List, counter: Int, suffix: *Char) {
    var instr = asm_write_instr(builder, op: null, 1);
    instr.args[0] = asm_mk_label_operand(counter, suffix);
}

func asm_write_raw(builder: *List, raw: *Char) {
    asm_write_instr(builder, raw, n_args: 0);
}

func asm_write_comment(builder: *List, fmt: *Char, ...args) {
    var sb = sb_new();
    sb_printf(sb, "// ");
    sb_vprintf(sb, fmt, args);
    var text = sb_finish(sb);
    asm_write_raw(builder, text);
}

func asm_add_comment_to_last(builder: *List, fmt: *Char, ...args) {
    var sb = sb_new();
    sb_printf(sb, "// ");
    sb_vprintf(sb, fmt, args);
    var text = sb_finish(sb);
    var instr: *AsmInstr = list_get(builder, list_len(builder) - 1);
    instr.comment = text;
}

//==============================================================================
//== Printing Assembly

func asm_print_reg(reg: Reg, width: Int) {
    if (reg == SP) {
        printf("sp");
    } else {
        if (width == 4) {
            printf("w%d", reg);
        } else {
            printf("x%d", reg);
        }
    }
}

func asm_print_operand(fun: *AsmFunc, operand: *AsmOperand) {
    if (operand.kind == AsmOperand_Reg) {
        var operand = operand as *AsmRegOperand;
        asm_print_reg(operand.reg, operand.width);
        if (operand.shift != 0) {
            printf(", lsl #%d", operand.shift);
        }
    } else if (operand.kind == AsmOperand_Mem) {
        var operand = operand as *AsmMemOperand;
        printf("[");
        for (var i = 0; i < operand.n_args; i += 1) {
            asm_print_operand(fun, operand.args[i]);
            if (i < operand.n_args - 1) {
                printf(", ");
            }
        }
        printf("]");
    } else if (operand.kind == AsmOperand_Int) {
        var operand = operand as *AsmIntOperand;
        printf("#%d", operand.value);
        if (operand.shift != 0) {
            printf(", lsl #%d", operand.shift);
        }
    } else if (operand.kind == AsmOperand_Label) {
        var operand = operand as *AsmLabelOperand;
        printf(".L%d", operand.counter);
        if (operand.suffix != null) {
            printf(".%s", operand.suffix);
        }
    } else if (operand.kind == AsmOperand_Global) {
        var operand = operand as *AsmGlobalOperand;
        printf("%s%s", operand.relocation_spec, operand.prefix);
        if (operand.counter != -1) {
            printf("%d", operand.counter);
        }
    } else if (operand.kind == AsmOperand_FpFromFrameStart) {
        var operand = operand as *AsmFpFromFrameStartOperand;
        printf("#%d", fun.spills_size + operand.offset);
    } else if (operand.kind == AsmOperand_SpFromFp) {
        var operand = operand as *AsmSpFromFpOperand;
        printf("#%d", fun.frame_size - fun.spills_size + operand.fp_offset);
    } else if (operand.kind == AsmOperand_Raw) {
        var operand = operand as *AsmRawOperand;
        printf("%s", operand.op);
    }
}

func asm_print_instr(fun: *AsmFunc, instr: *AsmInstr) {
    // special case for labels:
    if (instr.op == null) {
        asm_print_operand(fun, instr.args[0]);
        printf(":\n");
        return;
    }

    printf("  %s", instr.op);

    for (var i = 0; i < instr.n_args; i += 1) {
        var arg = instr.args[i];

        if (i == 0) {
            printf(" ");
        } else {
            printf(", ");
        }
        asm_print_operand(fun, arg);
    }

    if (instr.comment != null) {
        printf("  %s", instr.comment);
    }

    printf("\n");
}

func asm_print_func(fun: *AsmFunc) {
    var n_spills = fun.n_spills;
    var spills = fun.spills;
    var spills_size = fun.spills_size;

    printf("  .text\n");
    printf("  .align 2\n");
    printf("  .global %s\n", fun.name);
    printf("%s:\n", fun.name);

    // prologue
    printf("  // prologue\n");
    printf("  sub  sp, sp, #%d\n", spills_size);
    for (var i = 0; i < n_spills; i += 2) {
        if (i + 1 < n_spills) {
            printf("  stp  x%d, x%d, [sp, #%d]\n", spills[i], spills[i + 1], i * 8);
        } else {
            printf("  str  x%d, [sp, #%d]\n", spills[i], i * 8);
        }
    }
    printf("  mov  x29, sp\n");
    printf("  sub  sp, sp, #%d\n", fun.frame_size - spills_size);

    var n_instrs = list_len(fun.instrs);
    for (var i = 0; i < n_instrs; i += 1) {
        var instr: *AsmInstr = list_get(fun.instrs, i);
        asm_print_instr(fun, instr);
    }

    // epilogue
    printf("  // epilogue\n");
    printf("  add  sp, sp, #%d\n", fun.frame_size - spills_size);
    for (var i = 0; i < n_spills; i += 2) {
        if (i + 1 < n_spills) {
            printf("  ldp  x%d, x%d, [sp, #%d]\n", spills[i], spills[i + 1], i * 8);
        } else {
            printf("  ldr  x%d, [sp, #%d]\n", spills[i], i * 8);
        }
    }
    printf("  add  sp, sp, #%d\n", spills_size);
    printf("  ret\n");
}

//==============================================================================
//== Codegen Context

struct RegReservation {
    did_spill: Bool,
}

struct RegState {
    reservations: *List, // List<RegReservation>
    is_locked: Bool,
    n_writes: Int,
}

struct LoopCtx {
    step_label: Int,
    done_label: Int,
}

struct Slot {
    fp_offset: Int,
}

struct CodegenCtx {
    // execution context
    current_func: *FuncSym,
    current_loop: LoopCtx,
    current_call_layout: CallLayout,

    // varargs area
    saved_varargs_gr_top_fp_offset: Int,
    saved_varargs_gr_size: Int,

    // slot area
    n_slots: Int,
    slots: *Slot,

    // scratch area
    scratch_fp_offset: Int,
    current_scratch_size: Int,
    max_scratch_size: Int,

    // argument passing area
    max_arg_pass_size: Int,

    // labels
    n_labels: Int,
    ret_label: Int,

    // strings
    strings: *List, // List<*StringBuffer>

    // asm builder
    builder: *List,

    // register allocation
    regs: [RegState; N_REGS],
}

func next_label(ctx: *CodegenCtx): Int {
    var label = ctx.n_labels;
    ctx.n_labels += 1;
    return label;
}

func define_global_string(ctx: *CodegenCtx, value: *StringBuffer, xd: Reg): Int {
    list_push(ctx.strings, value);
    return list_len(ctx.strings) - 1;
}

//==============================================================================
//== Codegen

func write_push(ctx: *CodegenCtx, reg: Reg) {
    ctx.current_scratch_size += 8;
    ctx.max_scratch_size = int_max(ctx.max_scratch_size, ctx.current_scratch_size);

    // str {reg}, [sp, #{stack_size + fp_offset}]
    var fp_offset = ctx.scratch_fp_offset - ctx.current_scratch_size;
    asm_write_str_r_sp_from_fp(ctx.builder, width: 8, reg, fp_offset);
}

func write_pop(ctx: *CodegenCtx, reg: Reg) {
    // ldr {reg}, [sp, #{stack_size + fp_offset}]
    var fp_offset = ctx.scratch_fp_offset - ctx.current_scratch_size;
    asm_write_ldrs_r_sp_from_fp(ctx.builder, width: 8, reg, fp_offset);

    ctx.current_scratch_size -= 8;
}

func lock_reg(ctx: *CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    state.is_locked = true;
    state.n_writes += 1;
}

func unlock_reg(ctx: *CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    assert(state.is_locked, "unlock_reg: register should be locked.");
    state.is_locked = false;
}

func spill_reg(ctx: *CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    assert(state.is_locked, "spill_reg: register should be locked.");
    write_push(ctx, reg);
    unlock_reg(ctx, reg);

    asm_add_comment_to_last(ctx.builder, "8 byte spill");
}

func unspill_reg(ctx: *CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    assert(!state.is_locked, "unspill_reg: register should not be locked.");
    write_pop(ctx, reg);
    lock_reg(ctx, reg);

    asm_add_comment_to_last(ctx.builder, "8 byte reload");
}

func reserve_reg(ctx: *CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    var spill_needed = state.is_locked;
    if (spill_needed) {
        spill_reg(ctx, reg);
    }

    var reservation: *RegReservation = calloc(1, sizeof(RegReservation));
    *reservation = RegReservation { did_spill: spill_needed };
    list_push(state.reservations, reservation);
}

func free_reg(ctx: *CodegenCtx, reg: Reg) {
    var state = &ctx.regs[reg as Int];
    var reservation: *RegReservation = list_pop(state.reservations);
    var did_spill = reservation.did_spill;
    free(reservation);

    if (state.is_locked) {
        unlock_reg(ctx, reg);
    }
    if (did_spill) {
        unspill_reg(ctx, reg);
    }
}

func next_reg_except_2(ctx: *CodegenCtx, x1: Reg, x2: Reg): Reg {
    /*
        Register usage:
        - SP The Stack Pointer.
        - r30 / LR - The Link Register.
        - r29 / FP - The Frame Pointer
        - r19...r28 Callee-saved registers
        - r18 The Platform Register.
        - r17 IP1 The second intra-procedure-call temporary register.
        - r16 IP0 The first intra-procedure-call scratch register.
        - r9...r15 Temporary registers
        - r8 Indirect result location register
        - r0...r7 Parameter/result registers
    */

    var priority: [Reg; 30] = [
        Reg_X0, Reg_X1, Reg_X2, Reg_X3, Reg_X4, Reg_X5, Reg_X6, Reg_X7,
        Reg_X8,
        Reg_X9, Reg_X10, Reg_X11, Reg_X12, Reg_X13, Reg_X14, Reg_X15,
        Reg_X19, Reg_X20, Reg_X21, Reg_X22, Reg_X23, Reg_X24, Reg_X25, Reg_X26, Reg_X27, Reg_X28,
        Reg_X16, Reg_X17, Reg_X18,
        Reg_X30,
    ];

    for (var i = 0; i < 30; i += 1) {
        var reg = priority[i];
        if (reg != x1 && reg != x2) {
            var state = &ctx.regs[reg as Int];
            if (!state.is_locked) {
                reserve_reg(ctx, reg);
                return reg;
            }
        }
    }

    unreachable("next_reg_except_2: no free registers available.");
}

func next_reg_except_1(ctx: *CodegenCtx, x1: Reg): Reg {
    return next_reg_except_2(ctx, x1, x2: x1);
}

func next_reg(ctx: *CodegenCtx): Reg {
    return next_reg_except_2(ctx, x1: SP, x2: SP);
}

func spill_before_call(ctx: *CodegenCtx) {
    for (var i = 0; i < N_REGS; i += 1) {
        if (!reg_is_callee_saved(i as Reg)) {
            reserve_reg(ctx, i as Reg);
        }
    }
}

func unspill_after_call(ctx: *CodegenCtx) {
    for (var i = N_REGS - 1; i >= 0; i -= 1) {
        if (!reg_is_callee_saved(i as Reg)) {
            free_reg(ctx, i as Reg);
        }
    }
}

func get_fp_offset_for_slot(ctx: *CodegenCtx, type: *Type, slot_id: Int): Int {
    return ctx.slots[slot_id].fp_offset;
}

// str {reg}, [fp, #{fp_offset + offset}]
func write_slot_store_partial(ctx: *CodegenCtx, type: *Type, slot_id: Int, offset: Int, reg: Reg) {
    var width = type_size(type);
    var fp_offset = get_fp_offset_for_slot(ctx, type, slot_id);
    asm_write_str_ri(ctx.builder, width: width, reg, FP, fp_offset + offset);
}

// str {reg}, [fp, #{fp_offset}]
func write_slot_store(ctx: *CodegenCtx, type: *Type, slot_id: Int, reg: Reg) {
    write_slot_store_partial(ctx, type, slot_id, offset: 0, reg);
}

// add {reg} fp, #{fp_offset}
func write_slot_addr(ctx: *CodegenCtx, type: *Type, slot_id: Int, reg: Reg) {
    var width = type_size(type);
    var fp_offset = get_fp_offset_for_slot(ctx, type, slot_id);
    asm_write_add_ri(ctx.builder, width: width, reg, FP, fp_offset);
}

// ldr {reg}, [sp, #{stack_size + fp_offset}]
func write_slot_read(ctx: *CodegenCtx, type: *Type, slot_id: Int, xd: Reg) {
    var width = type_size(type);
    var fp_offset = get_fp_offset_for_slot(ctx, type, slot_id);
    asm_write_ldrs_r_sp_from_fp(ctx.builder, width: width, xd, fp_offset);
}

func write_sign_extend(ctx: *CodegenCtx, source: *Type, xd: Reg, x1: Reg) {
    assert(is_scalar(source), "write_sign_extend: source should be a scalar.");
    if (is_int_or_enum(source, max_size: 4)) {
        var target_size = 8;
        var source_size = type_size(source);
        // {sxt} {xd}, {x1}
        asm_write_sxt(ctx.builder, target_size, source_size, xd, x1);
    } else {
        if (xd != x1) {
            // mov {xd}, {x1}
            asm_write_mov_r(ctx.builder, width: 8, xd, x1);
        }
    }
}

func asm_lower_expr(ctx: *CodegenCtx, e: *HirExpr, xd: Reg);

func asm_lower_expr_addr(ctx: *CodegenCtx, e: *HirExpr, xd: Reg);

func asm_lower_expr_binary(ctx: *CodegenCtx, op: *Char, e1: *HirExpr, e2: *HirExpr, xd: Reg) {
    var x1 = next_reg_except_1(ctx, xd);

    // xd <- ...
    asm_lower_expr(ctx, e1, xd);
    // x1 <- ...
    asm_lower_expr(ctx, e2, x1);
    // {op} xd, xd, x1
    asm_write_binary_op_rr(ctx.builder, op, xd, xd, x1);

    free_reg(ctx, x1);
}

func asm_lower_expr_cmp(ctx: *CodegenCtx, op: *Char, e1: *HirExpr, e2: *HirExpr, xd: Reg) {
    var x1 = next_reg_except_1(ctx, xd);

    // xd <- ...
    asm_lower_expr(ctx, e1, xd);
    // x1 <- ...
    asm_lower_expr(ctx, e2, x1);
    // cmp xd, x1
    asm_write_cmp_rr(ctx.builder, width: 8, xd, x1);
    // cset xd, op
    asm_write_cset(ctx.builder, width: 8, xd, op);

    free_reg(ctx, x1);
}

func asm_lower_expr_addr(ctx: *CodegenCtx, e: *HirExpr, xd: Reg) {
    if (e.kind == HirExpr_Var && (e as *HirVarExpr).sym.kind == Sym_Local) {
        var e = e as *HirVarExpr;
        var sym = e.sym as *LocalSym;

        if (sym.is_indirect) {
            write_slot_read(ctx, sym.type, sym.slot_id, xd);
            asm_add_comment_to_last(ctx.builder, "local '%s' (indirect) @ %d", sym.name, sym.slot_id);
        } else {
            write_slot_addr(ctx, sym.type, sym.slot_id, xd);
            asm_add_comment_to_last(ctx.builder, "local '%s' @ %d", sym.name, sym.slot_id);
        }
    } else if (e.kind == HirExpr_Var && (e as *HirVarExpr).sym.kind == Sym_Global) {
        var e = e as *HirVarExpr;
        var sym = e.sym as *GlobalSym;
        if (sym.is_defined) {
            // adrp {xd}, {name}
            // add {xd}, {xd}, :lo12:{name}
            asm_write_global_addr(ctx.builder, xd, sym.name);
        } else {
            // adrp {xd}, :got:{name}
            // ldr {xd}, [{xd}, :got_lo12:{name}]
            asm_write_got_global_addr(ctx.builder, xd, sym.name);
        }
    } else if (e.kind == HirExpr_Temp) {
        var e = e as *HirTempExpr;
        var temp = e.temp;

        write_slot_addr(ctx, temp.type, temp.slot_id, xd);
        asm_add_comment_to_last(ctx.builder, "temp @ %d", temp.slot_id);
    } else if (e.kind == HirExpr_Member) {
        var e = e as *HirMemberExpr;
        var fields = (e.left.type as *StructType).sym.fields;
        var field: *StructField = list_get(fields, e.field_index);

        asm_lower_expr_addr(ctx, e.left, xd);
        // add {xd}, {xd}, #{offset}
        asm_write_add_ri(ctx.builder, width: 8, xd, xd, field.offset);
        asm_add_comment_to_last(ctx.builder, "member '%s'", field.name);
    } else if (e.kind == HirExpr_Deref) {
        var e = e as *HirDerefExpr;
        asm_lower_expr(ctx, e.expr, xd);
    } else if (e.kind == HirExpr_Index) {
        var e = e as *HirIndexExpr;
        var x1 = next_reg_except_1(ctx, xd);
        var x2 = next_reg_except_2(ctx, xd, x1);
        if (e.indexee.type.kind == Type_Ptr) {
            // xd <- ...
            asm_lower_expr(ctx, e.indexee, xd);
            // x1 <- ...
            asm_lower_expr(ctx, e.index, x1);
        } else if (e.indexee.type.kind == Type_Arr) {
            // xd <- ...
            asm_lower_expr_addr(ctx, e.indexee, xd);
            // x1 <- ...
            asm_lower_expr(ctx, e.index, x1);
        } else {
            unreachable("asm_lower_expr_addr: indexee should be a pointer or an array.");
        }
        var elem_size = type_size(e.type);
        asm_write_mov_i(ctx.builder, width: 8, x2, elem_size);
        asm_write_binary_op_rrr(ctx.builder, "madd", xd, x1, x2, xd);
        free_reg(ctx, x1);
    } else {
        unreachable("asm_lower_expr_addr");
    }
    lock_reg(ctx, xd);
}

func asm_lower_seq_expr(ctx: *CodegenCtx, e: *HirSeqExpr, xd: Reg) {
    var first = e.first;
    var second = e.second;
    // x1 <- ...
    var x1 = next_reg_except_1(ctx, xd);
    asm_lower_expr(ctx, first, x1);
    free_reg(ctx, x1);
    // xd <- ...
    asm_lower_expr(ctx, second, xd);
}

func asm_lower_int_expr(ctx: *CodegenCtx, e: *HirIntExpr, xd: Reg) {
    var int_val = e.value;
    // mov {xd}, #{int_val}
    asm_write_mov_i(ctx.builder, width: 8, xd, int_val);
}

func asm_lower_str_expr(ctx: *CodegenCtx, e: *HirStrExpr, xd: Reg) {
    var string_index = define_global_string(ctx, e.value, xd);
    asm_write_string_addr(ctx.builder, xd, string_index);
}

func asm_lower_cond_expr(ctx: *CodegenCtx, e: *HirCondExpr, xd: Reg) {
    var cond = e.cond;
    var then_expr = e.then_expr;
    var else_expr = e.else_expr;

    var if_label = next_label(ctx);
    var then_label = next_label(ctx);
    var else_label = next_label(ctx);
    var end_label = next_label(ctx);

    // L.if:
    asm_write_label(ctx.builder, if_label, "if");
    // x0 <- ...
    asm_lower_expr(ctx, cond, xd);
    // cbz x0, L.else
    asm_write_cbz(ctx.builder, xd, else_label, "else");
    unlock_reg(ctx, xd);

    // L.then:
    asm_write_label(ctx.builder, then_label, "then");
    // {xd} <- ...
    asm_lower_expr(ctx, then_expr, xd);
    // b L.end
    asm_write_b(ctx.builder, end_label, "end");

    // L.else:
    unlock_reg(ctx, xd);
    asm_write_label(ctx.builder, else_label, "else");
    // {xd} <- ...
    asm_lower_expr(ctx, else_expr, xd);

    // L.end:
    asm_write_label(ctx.builder, end_label, "end");
}

func asm_lower_loop_expr(ctx: *CodegenCtx, e: *HirLoopExpr, xd: Reg) {
    var cond = e.cond;
    var body = e.body;
    var step = e.step;

    var while_label = next_label(ctx);
    var do_label = next_label(ctx);
    var step_label = next_label(ctx);
    var done_label = next_label(ctx);

    var outer_loop = ctx.current_loop;
    ctx.current_loop = LoopCtx { step_label, done_label };

    // L.while:
    asm_write_label(ctx.builder, while_label, "while");
    // x0 <- ...
    asm_lower_expr(ctx, cond, xd);
    // cbz x0, L.done
    asm_write_cbz(ctx.builder, xd, done_label, "done");
    unlock_reg(ctx, xd);

    // L.do:
    asm_write_label(ctx.builder, do_label, "do");
    // x0 <- ...
    asm_lower_expr(ctx, body, xd);
    unlock_reg(ctx, xd);

    // L.step:
    asm_write_label(ctx.builder, step_label, "step");
    // x0 <- ...
    asm_lower_expr(ctx, step, xd);
    unlock_reg(ctx, xd);
    // b L.while
    asm_write_b(ctx.builder, while_label, "while");

    // L.done:
    asm_write_label(ctx.builder, done_label, "done");

    ctx.current_loop = outer_loop;
}

func asm_lower_jump_expr(ctx: *CodegenCtx, e: *HirJumpExpr, xd: Reg) {
    var is_break = e.is_break;
    if (is_break) {
        // b .L.done
        asm_write_b(ctx.builder, ctx.current_loop.done_label, "done");
    } else {
        // b .L.step
        asm_write_b(ctx.builder, ctx.current_loop.step_label, "step");
    }
}

func asm_lower_return_expr(ctx: *CodegenCtx, e: *HirReturnExpr, xd: Reg) {
    if (e.expr) {
        var ret_loc = &ctx.current_call_layout.ret_loc;

        if (ret_loc.is_indirect) {
            assert(hir_is_lvalue(e.expr), "asm_lower_return_expr: composite return should be an lvalue.");
            // x1 <- &...
            // ldr x0, [fp, #-8] // restore indirect result location
            // mov x2, #{type size}
            // bl memcpy

            spill_before_call(ctx);

            asm_lower_expr_addr(ctx, e.expr, Reg_X1);
            asm_write_ldrs_ri(ctx.builder, width: 8, Reg_X0, FP, -8);
            asm_add_comment_to_last(ctx.builder, "restore indirect result location");
            asm_write_mov_i(ctx.builder, width: 8, Reg_X2, type_size(e.expr.type));
            asm_write_bl(ctx.builder, "memcpy");

            unspill_after_call(ctx);
        } else if (ret_loc.is_reg) {
            if (is_scalar(e.expr.type)) {
                // x0 <- ...

                reserve_reg(ctx, Reg_X0);
                asm_lower_expr(ctx, e.expr, Reg_X0);
                free_reg(ctx, Reg_X0);
            } else {
                assert(hir_is_lvalue(e.expr), "asm_lower_return_expr: composite return should be an lvalue.");
                // x0 <- &...
                // ldr xn, [x0, #{n * 8}]
                // ...
                // ldr x0, [x0, #{0 * 8}]

                reserve_reg(ctx, Reg_X0);
                asm_lower_expr_addr(ctx, e.expr, Reg_X0);
                for (var i = ret_loc.n_regs - 1; i >= 0; i -= 1) {
                    var reg = (Reg_X0 as Int + i) as Reg;
                    asm_write_ldrs_ri(ctx.builder, width: 8, reg, Reg_X0, offset: i * 8);
                }
                free_reg(ctx, Reg_X0);
            }
        } else {
            unreachable("asm_lower_return_expr: invalid return location.");
        }
    }

    // b .L.ret
    asm_write_b(ctx.builder, ctx.ret_label, "ret");
}

func asm_lower_binary_op_expr(ctx: *CodegenCtx, e: *HirBinaryOpExpr, xd: Reg) {
    var op = e.op;
    var e1 = e.left;
    var e2 = e.right;
    var type = e.type;

    if (op == HirOp_Or) {
        asm_lower_expr_binary(ctx, "orr", e1, e2, xd);
    } else if (op == HirOp_Xor) {
        asm_lower_expr_binary(ctx, "eor", e1, e2, xd);
    } else if (op == HirOp_And) {
        asm_lower_expr_binary(ctx, "and", e1, e2, xd);
    } else if (op == HirOp_Shl) {
        asm_lower_expr_binary(ctx, "lsl", e1, e2, xd);
    } else if (op == HirOp_Shr) {
        asm_lower_expr_binary(ctx, "lsr", e1, e2, xd);
    } else if (op == HirOp_Eq) {
        asm_lower_expr_cmp(ctx, "eq", e1, e2, xd);
    } else if (op == HirOp_Ne) {
        asm_lower_expr_cmp(ctx, "ne", e1, e2, xd);
    } else if (op == HirOp_Lt) {
        asm_lower_expr_cmp(ctx, "lt", e1, e2, xd);
    } else if (op == HirOp_Le) {
        asm_lower_expr_cmp(ctx, "le", e1, e2, xd);
    } else if (op == HirOp_Gt) {
        asm_lower_expr_cmp(ctx, "gt", e1, e2, xd);
    } else if (op == HirOp_Ge) {
        asm_lower_expr_cmp(ctx, "ge", e1, e2, xd);
    } else if (op == HirOp_Add) {
        asm_lower_expr_binary(ctx, "add", e1, e2, xd);
    } else if (op == HirOp_Sub) {
        asm_lower_expr_binary(ctx, "sub", e1, e2, xd);
    } else if (op == HirOp_Mul) {
        asm_lower_expr_binary(ctx, "mul", e1, e2, xd);
    } else if (op == HirOp_Div) {
        asm_lower_expr_binary(ctx, "sdiv", e1, e2, xd);
    } else if (op == HirOp_Rem) {
        var x1 = next_reg_except_1(ctx, xd);
        var x2 = next_reg_except_2(ctx, xd, x1);
        // x1 <- ...
        asm_lower_expr(ctx, e1, x1);
        // x2 <- ...
        asm_lower_expr(ctx, e2, x2);
        // sdiv x0, x1, x2
        asm_write_binary_op_rr(ctx.builder, "sdiv", xd, x1, x2);
        // msub {xd}, x0, x2, x1
        asm_write_binary_op_rrr(ctx.builder, "msub", xd, xd, x2, x1);

        free_reg(ctx, x2);
        free_reg(ctx, x1);
    } else {
        unreachable("asm_lower_binary_op_expr");
    }
}

func asm_lower_call_expr(ctx: *CodegenCtx, e: *HirCallExpr, xd: Reg, is_composite_assign: Bool) {
    var layout: CallLayout;
    layout = get_call_layout_for_call(e);

    var is_reg_ret_scalar = layout.ret_loc.is_reg && is_scalar(e.type);
    var is_reg_ret_composite = layout.ret_loc.is_reg && is_composite(e.type);
    var is_indirect_ret = layout.ret_loc.is_indirect;

    asm_write_comment(ctx.builder, "prepare call to %s", e.callee.name);

    spill_before_call(ctx);

    if (is_indirect_ret) {
        assert(is_composite_assign, "asm_lower_call_expr: destination should be address.");

        // Prepare the indirect return location
        // mov x8 xd
        asm_write_mov_r(ctx.builder, width: 8, Reg_X8, xd);
        lock_reg(ctx, Reg_X8);
    } else if (is_reg_ret_composite) {
        assert(is_composite_assign, "asm_lower_call_expr: destination should be address.");

        // Save destination for later
        write_push(ctx, xd);
    }

    for (var i = 0; i < layout.n_args; i += 1) {
        var arg: *HirExpr = list_get(e.args, i);
        var arg_loc = &layout.arg_locs[i];
        if (arg_loc.is_reg && is_scalar(arg.type)) {
            // Pass scalar in register.
            // x{i} <- ...
            asm_lower_expr(ctx, arg, arg_loc.reg as Reg);
        } else if (arg_loc.is_reg && is_pass_by_ptr(arg.type) && hir_is_lvalue(arg)) {
            // Pass composite by pointer in register.
            // x{i} <- &...
            asm_lower_expr_addr(ctx, arg, arg_loc.reg as Reg);
        } else if (arg_loc.is_reg && hir_is_lvalue(arg)) {
            // Pass 8 or 16 byte aggregate in consecutive registers.
            //
            // x{lo} <- &...
            // ldr x{lo+n} [x{lo}, #8 * n]
            // ...
            // ldr x{lo+0} [x{lo}, #8 * 0]
            var lo = arg_loc.reg;
            var n = arg_loc.n_regs;
            asm_lower_expr_addr(ctx, arg, lo as Reg);
            for (var j = n - 1; j >= 0; j -= 1) {
                asm_write_ldrs_ri(ctx.builder, width: 8, (lo + j) as Reg, lo as Reg, 8 * j);
            }
        } else if (arg_loc.is_stack && is_scalar(arg.type)) {
            // Pass scalar on stack.
            // {x1} <- ...
            // str {x1}, [sp, #{offset}]
            var x1 = next_reg_except_1(ctx, SP);
            asm_lower_expr(ctx, arg, x1);
            asm_write_str_ri(ctx.builder, width: 8, x1, SP, arg_loc.offset);
            free_reg(ctx, x1);
        } else if (arg_loc.is_stack && hir_is_lvalue(arg)) {
            // Pass aggregate on stack.
            //
            // x1 <- &...
            // add x0, sp, #{offset}
            // mov x2, #{size}
            // bl memcpy
            spill_before_call(ctx);
            asm_lower_expr_addr(ctx, arg, Reg_X1);
            asm_write_add_ri(ctx.builder, width: 8, Reg_X0, SP, arg_loc.offset);
            asm_write_mov_i(ctx.builder, width: 8, Reg_X2, type_size(arg.type));
            asm_write_bl(ctx.builder, "memcpy");
            unspill_after_call(ctx);
        } else {
            unreachable("asm_lower_call_expr");
        }
    }

    // bl {name}
    asm_write_bl(ctx.builder, e.callee.name);

    var xm = layout.ret_loc.n_regs as Reg;

    if (is_reg_ret_composite) {
        // Restore destination
        write_pop(ctx, xm);
    }

    if (is_reg_ret_scalar) {
        write_sign_extend(ctx, e.type, xd, Reg_X0);
    } else if (is_reg_ret_composite) {
        // xm <- &...
        // str x0, [xm]
        // ...
        // str xn, [xm, #8 * n]
        var n_regs = layout.ret_loc.n_regs;
        for (var i = 0; i < n_regs; i += 1) {
            var reg = (Reg_X0 as Int + i) as Reg;
            asm_write_str_ri(ctx.builder, width: 8, reg, xm, 8 * i);
        }
    } else if (layout.ret_loc.is_reg) {
        assert(false, "asm_lower_call_expr: Only scalar and composite types can be returned in registers.");
    } else if (is_indirect_ret) {
        // Nothing to do for the caller
    }

    unspill_after_call(ctx);

    asm_write_comment(ctx.builder, "call to %s done", e.callee.name);

    ctx.max_arg_pass_size = int_max(ctx.max_arg_pass_size, layout.stack_space);

    call_layout_drop(&layout);
}

func asm_lower_addr_expr(ctx: *CodegenCtx, e: *HirAddrExpr, xd: Reg) {
    var subexpr = e.expr;
    asm_lower_expr_addr(ctx, subexpr, xd);
}

func asm_lower_assign_expr(ctx: *CodegenCtx, e: *HirAssignExpr, xd: Reg) {
    var dst = e.dst;
    var src = e.src;
    if (is_scalar(dst.type)) {
        var x1 = next_reg_except_1(ctx, xd);
        var x2 = next_reg_except_2(ctx, xd, x1);

        // x1 <- ...
        asm_lower_expr_addr(ctx, dst, x1);
        // x2 <- ...
        asm_lower_expr(ctx, src, x2);
        // strx x2, [x1]
        asm_write_str_r(ctx.builder, type_size(dst.type), x2, x1);

        free_reg(ctx, x2);
        free_reg(ctx, x1);
    } else if (is_composite(dst.type) && src.kind == HirExpr_Call) {
        asm_lower_expr_addr(ctx, dst, xd);
        unlock_reg(ctx, xd);
        asm_lower_call_expr(ctx, src as *HirCallExpr, xd, is_composite_assign: true);
    } else if (is_composite(dst.type)) {
        spill_before_call(ctx);

        // x0, x1 <- ...
        asm_lower_expr_addr(ctx, dst, Reg_X0);
        asm_lower_expr_addr(ctx, src, Reg_X1);
        // mov x2, #{size}
        asm_write_mov_i(ctx.builder, width: 8, Reg_X2, type_size(dst.type));
        // bl memcpy
        asm_write_bl(ctx.builder, "memcpy");

        unspill_after_call(ctx);
    } else {
        unreachable("asm_lower_assign_expr");
    }
}

func asm_lower_cast_expr(ctx: *CodegenCtx, e: *HirCastExpr, xd: Reg) {
    var subexpr = e.expr;
    var target = e.type;
    var source = subexpr.type;
    asm_lower_expr(ctx, subexpr, xd);
    assert(is_scalar(target) && is_scalar(source), "asm_lower_expr: <cast> should have scalar types.");
    if (type_size(target) < type_size(source)) {
        write_sign_extend(ctx, target, xd, xd);
    } else {
        // no-op
    }
}

func asm_lower_expr(ctx: *CodegenCtx, e: *HirExpr, xd: Reg) {
    if (hir_is_lvalue(e)) {
        assert(is_scalar(e.type), "asm_lower_expr: lvalue must evaluate to a scalar.");
        asm_lower_expr_addr(ctx, e, xd);
        // ldr {xd}, [{xd}]
        asm_write_ldrs_r(ctx.builder, type_size(e.type), xd, xd);
    } else if (e.kind == HirExpr_Skip) {
        // nop
    } else if (e.kind == HirExpr_Seq) {
        asm_lower_seq_expr(ctx, e as *HirSeqExpr, xd);
    } else if (e.kind == HirExpr_Int) {
        asm_lower_int_expr(ctx, e as *HirIntExpr, xd);
    } else if (e.kind == HirExpr_Str) {
        asm_lower_str_expr(ctx, e as *HirStrExpr, xd);
    } else if (e.kind == HirExpr_Cond) {
        asm_lower_cond_expr(ctx, e as *HirCondExpr, xd);
    } else if (e.kind == HirExpr_Loop) {
        asm_lower_loop_expr(ctx, e as *HirLoopExpr, xd);
    } else if (e.kind == HirExpr_Jump) {
        asm_lower_jump_expr(ctx, e as *HirJumpExpr, xd);
    } else if (e.kind == HirExpr_Return) {
        asm_lower_return_expr(ctx, e as *HirReturnExpr, xd);
    } else if (e.kind == HirExpr_BinaryOp) {
        asm_lower_binary_op_expr(ctx, e as *HirBinaryOpExpr, xd);
    } else if (e.kind == HirExpr_Call) {
        asm_lower_call_expr(ctx, e as *HirCallExpr, xd, is_composite_assign: false);
    } else if (e.kind == HirExpr_Addr) {
        asm_lower_addr_expr(ctx, e as *HirAddrExpr, xd);
    } else if (e.kind == HirExpr_Assign) {
        asm_lower_assign_expr(ctx, e as *HirAssignExpr, xd);
    } else if (e.kind == HirExpr_Cast) {
        asm_lower_cast_expr(ctx, e as *HirCastExpr, xd);
    } else if (e.kind == HirExpr_Unreachable) {
        asm_write_comment(ctx.builder, "<- unreachable");
    } else {
        unreachable("asm_lower_expr");
    }
    lock_reg(ctx, xd);
}

func asm_lower_func(ctx: *CodegenCtx, sym: *FuncSym, hir_body: *HirExpr) {
    var layout = &ctx.current_call_layout;

    // Save indirect result location
    if (layout.ret_loc.is_indirect) {
        // str x8, [fp, #-8]
        asm_write_str_ri(ctx.builder, width: 8, Reg_X8, FP, -8);
        asm_add_comment_to_last(ctx.builder, "save indirect result location");
    }

    // Save varargs
    if (sym.is_variadic) {
        var gr_base = ctx.saved_varargs_gr_top_fp_offset - ctx.saved_varargs_gr_size;
        for (var i = layout.next_gpr; i < 8; i += 1) {
            var src_reg = i as Reg;
            var fp_offset = gr_base + 8 * (i - layout.next_gpr);
            // str x{i}, [fp, #{fp_offset}]
            asm_write_str_ri(ctx.builder, width: 8, src_reg, FP, fp_offset);
        }
    }

    // Copy arguments to stack slots
    for (var i = 0; i < layout.n_args; i += 1) {
        var param: *FuncParam = list_get(sym.params, i);
        var local: *LocalSym = list_get(sym.locals, i);
        var arg_loc = &layout.arg_locs[i];
        if (arg_loc.is_reg) {
            for (var j = 0; j < arg_loc.n_regs; j += 1) {
                var reg = (arg_loc.reg + j) as Reg;
                write_slot_store_partial(ctx, param.type, local.slot_id, 8 * j, reg);
            }
            local.is_indirect = is_pass_by_ptr(param.type);
        } else if (type_size(param.type) <= 8) {
            // ldr x0, [fp, #{spills_size + arg_offset}]
            asm_write_ldrs_r_fp_from_frame_start(ctx.builder, width: 8, Reg_X0, arg_loc.offset);
            // str x0, [fp, #{fp_offset}]
            write_slot_store(ctx, param.type, local.slot_id, Reg_X0);
        } else {
            // Passed on stack. Must be copied to local slot. Handled separately below
            // to make sure calling memcpy doesn't clobber the remaining unread arguments.
        }
    }

    // Create va_list if needed
    if (sym.rest_param_name) {
        var local: *LocalSym = list_get(sym.locals, layout.n_args);
        var slot_id = local.slot_id;
        var fp_offset = get_fp_offset_for_slot(ctx, mk_rest_param_type(), slot_id);

        // va_list layout:
        //
        // struct va_list {
        //     stack: *Void,
        //     gr_top: *Void,
        //     vr_top: *Void,
        //     gr_offs: Int32,
        //     vr_offs: Int32,
        // }

        asm_write_comment(ctx.builder, "initialize va_list");

        // x0 <- &local
        write_slot_addr(ctx, mk_rest_param_type(), slot_id, Reg_X0);
        // ; va_list.stack
        // add x1, FP, #{spills_size}
        // str x1, [x0, #0]
        asm_write_add_r_fp_from_frame_start(ctx.builder, width: 8, Reg_X1, 0);
        asm_write_str_ri(ctx.builder, width: 8, Reg_X1, Reg_X0, 0);
        // ; va_list.gr_top
        // add x1, FP, #{saved_varargs_gr_top_fp_offset}
        // str x1, [x0, #8]
        asm_write_add_ri(ctx.builder, width: 8, Reg_X1, FP, ctx.saved_varargs_gr_top_fp_offset);
        asm_write_str_ri(ctx.builder, width: 8, Reg_X1, Reg_X0, 8);
        // ; va_list.vr_top
        // add x1, FP, #{saved_varargs_gr_top_fp_offset - saved_varargs_gr_size}
        // str x1, [x0, #16]
        asm_write_add_ri(ctx.builder, width: 8, Reg_X1, FP, ctx.saved_varargs_gr_top_fp_offset - ctx.saved_varargs_gr_size);
        asm_write_str_ri(ctx.builder, width: 8, Reg_X1, Reg_X0, 16);
        // ; va_list.gr_offs
        // mov x1, #{-saved_gp_varargs_size}
        // str x1, [x0, #24]
        asm_write_mov_i(ctx.builder, width: 4, Reg_X1, -ctx.saved_varargs_gr_size);
        asm_write_str_ri(ctx.builder, width: 4, Reg_X1, Reg_X0, 24);
        // ; va_list.vr_offs
        // mov x1, #0
        // str x1, [x0, #28]
        asm_write_mov_i(ctx.builder, width: 4, Reg_X1, 0);
        asm_write_str_ri(ctx.builder, width: 4, Reg_X1, Reg_X0, 28);

        asm_write_comment(ctx.builder, "va_list initialized");
    }


    for (var i = 0; i < layout.n_args; i += 1) {
        var param: *FuncParam = list_get(sym.params, i);
        var local: *LocalSym = list_get(sym.locals, i);
        var arg_loc = &layout.arg_locs[i];

        if (arg_loc.is_reg) {
            // Already handled
        } else if (type_size(param.type) <= 8) {
            // Already handled
        } else {
            // add x0, fp, #{??}
            // add x1, fp, #{spills_size + arg_offset}
            // mov x2, #{size}
            // bl memcpy

            write_slot_addr(ctx, param.type, local.slot_id, Reg_X0);
            asm_write_add_r_fp_from_frame_start(ctx.builder, width: 8, Reg_X1, arg_loc.offset);
            asm_write_mov_i(ctx.builder, width: 8, Reg_X2, type_size(param.type));
            asm_write_bl(ctx.builder, "memcpy");
        }
    }

    asm_lower_expr(ctx, hir_body, Reg_X0);

    // L.ret:
    asm_write_label(ctx.builder, ctx.ret_label, "ret");
}

func layout_slots(ctx: *CodegenCtx, start_fp_offset: Int): Int {
    var sym = ctx.current_func;

    var n_locals = list_len(sym.locals);
    var n_temps = list_len(sym.temps);
    var n_slots = n_locals + n_temps;
    var slots: *Slot = calloc(n_slots, sizeof(Slot));

    var next_fp_offset = start_fp_offset;
    for (var i = 0; i < n_locals; i += 1) {
        var local: *LocalSym = list_get(sym.locals, i);
        next_fp_offset = -align_up(-next_fp_offset + type_size(local.type), type_align(local.type));
        slots[i] = Slot { fp_offset: next_fp_offset };
    }
    for (var i = 0; i < n_temps; i += 1) {
        var temp: *HirTemp = list_get(sym.temps, i);
        next_fp_offset = -align_up(-next_fp_offset + type_size(temp.type), type_align(temp.type));
        slots[n_locals + i] = Slot { fp_offset: next_fp_offset };
    }

    ctx.n_slots = n_slots;
    ctx.slots = slots;

    return start_fp_offset - next_fp_offset;
}

func emit_func(ctx: *CodegenCtx, sym: *FuncSym) {
    var hir_body = hir_lower(sym, sym.body);

    ctx.current_func = sym;
    ctx.ret_label = next_label(ctx);
    ctx.current_call_layout = get_call_layout_for_func(sym);

    var next_fp_offset = 0;

    if (ctx.current_call_layout.ret_loc.is_indirect) {
        next_fp_offset -= 8;
    }

    if (sym.is_variadic && ctx.current_call_layout.next_gpr < 8) {
        ctx.saved_varargs_gr_top_fp_offset = next_fp_offset;
        var n_unallocated_gprs = 8 - ctx.current_call_layout.next_gpr;
        next_fp_offset -= 8 * n_unallocated_gprs;
        ctx.saved_varargs_gr_size = 8 * n_unallocated_gprs;
    }

    var slots_size = layout_slots(ctx, next_fp_offset);
    next_fp_offset -= slots_size;

    ctx.scratch_fp_offset = -align_up(-next_fp_offset, 8);
    next_fp_offset = ctx.scratch_fp_offset;

    ctx.builder = list_new();

    memset(&ctx.regs, 0, sizeof([RegState; N_REGS]));
    for (var i = 0; i < N_REGS; i += 1) {
        ctx.regs[i].reservations = list_new();
    }

    asm_write_comment(ctx.builder, "body");
    asm_lower_func(ctx, sym, hir_body);

    var spills: [Reg; N_REGS];

    spills[0] = Reg_X29;
    spills[1] = Reg_X30;
    var n_spills = 2; // x29, x30

    for (var i = 0; i < N_REGS; i += 1) {
        var is_reg_used = ctx.regs[i].n_writes > 0;
        if (is_reg_used && reg_is_callee_saved(i as Reg)) {
            spills[n_spills] = i as Reg;
            n_spills += 1;
        }
    }

    var spills_size = align_up(n_spills * 8, 16);

    var frame_size = align_up(spills_size + -next_fp_offset + ctx.max_scratch_size + ctx.max_arg_pass_size, 16);

    asm_print_func(&AsmFunc {
        name: sym.name,
        instrs: ctx.builder,
        frame_size: frame_size,
        spills: spills,
        n_spills: n_spills,
        spills_size: spills_size,
    });

    call_layout_drop(&ctx.current_call_layout);
}

func emit_global(ctx: *CodegenCtx, sym: *GlobalSym) {
    var type = sym.type;
    printf("  .global %s\n", sym.name);
    printf("  .bss\n");
    printf("  .align %d\n", type_align(type));
    printf("%s:\n", sym.name);
    printf("  .zero %d\n", type_size(type));
}

func emit_sym(ctx: *CodegenCtx, sym: *Sym) {
    if (!sym_is_defined(sym)) {
        return;
    }
    if (sym.kind == Sym_Func) {
        emit_func(ctx, sym as *FuncSym);
    } else if (sym.kind == Sym_Global) {
        emit_global(ctx, sym as *GlobalSym);
    }
}

func emit_string(id: Int, str: *StringBuffer) {
    printf("  .text\n");
    printf("  .section .rodata\n");
    printf("  .align 3\n");
    printf(".L.str.%d:\n", id);
    printf("  .string \"");
    var i = 0;
    for (var i = 0; i < sb_len(str); i += 1) {
        var c = sb_get(str, i);
        if (!is_print(c) || c == '\"' || c == '\\') {
            printf("\\%03o", c);
        } else {
            printf("%c", c);
        }
    }
    printf("\"\n");
}

func emit_program(syms: *List) {
    var ctx = calloc(1, sizeof(CodegenCtx)) as *CodegenCtx;
    ctx.strings = list_new();

    for (var i = 0; i < list_len(syms); i += 1) {
        var sym = list_get(syms, i);
        emit_sym(ctx, sym);
    }

    var n_strings = list_len(ctx.strings);
    for (var i = 0; i < n_strings; i += 1) {
        emit_string(i, list_get(ctx.strings, i));
    }
}
